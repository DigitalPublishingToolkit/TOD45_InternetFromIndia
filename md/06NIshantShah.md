---
Pr-id: MoneyLab
P-id: INC Reader
A-id: 10
Type: article
Book-type: anthology
Anthology item: article
Item-id: unique no.
Article-title: title of the article
Article-status: accepted
Author: name(s) of author(s)
Author-email:   corresponding address
Author-bio:  about the author
Abstract:   short description of the article (100 words)
Keywords:   50 keywords for search and indexing
Rights: CC BY-NC 4.0
...


# The Insidiousness of Information Overload 

### Nishant Shah

## Introductory Note

In 2018, there was still a breathless excitement about information
access, expansion, and bounty, leading to futurescaping scenarios of
interactive smart cities, self-driving autonomous vehicles, and 5G
inspired virtual environments that we shall all live and interact in.
The promise of the internet (and all things included in that catch-all
term) was still expressed in the boundless, limitless, and accelerated
information which forms the basis of Big Data dreams as well. However,
somewhere around that time, there also emerged a different conversation
– about excessive data streams, relentless notifications, algorithmic
manipulation, and the capacity to discern the veracity, validity, or
value of the information that was circulating so fast that any
meaningful interaction with it became difficult. The proliferation of
‘fake news’, in particular, made it clear that the extraordinary and
uncontrolled spread of digital information had suddenly erupted as a
critical and unforeseen problem.

In addition, with more and more people finding a voice online – we were
globally celebrating some powerful hashtags like \#metoo and their local
consequences – there was also increased backlash, online violence, and
abuse directed at them through institutional and informal organization.
The excess of information, or information overload, was no longer just
an information design and data management question; it had become a
weaponized mode of address, leading to silencing, intimidation, and
harm. While we were processing these
questions, the COVID-19 pandemic shrunk our lives into rectangles on
screens and the weariness of increased digital engagement – encapsulated
in the cultural zeitgeist as ‘Zoom fatigue’ – led to people’s
disengagement from decision-making. People were relying more on
automated algorithmic structures to curate the information that they
engaged with. We were slowly recognizing the unbearable lightness of
digitization and the insufferable weight of information, realizing that
the thrill of plenty was now manifesting as the tyranny of overload.

Information overload became the unspoken state of digital being and it
took us by surprise. The assurances of search
engines, database management, algorithmic curation, peer-2-peer
dissemination, wisdom of crowds, intuitive information shaping, and
emancipation from the task of remembering everything by putting it into
storage seemed to have done their rhetorical work so well that when we
started recognizing information overload, it felt like it was new,
sudden, and unexpected, and we didn’t quite know how it happened. This
essay is an attempt to first recognize and identify this state of
information overload as a cultural and political, not just
technological, question. It examines the cost of being in this perpetual
state of crisis of informational overload and how it shapes our
conversations about action and activism. More urgently, it refuses the
framework of unexpectedness and surprise and shows how information
overload is not the bug but rather the feature of computational network
design, and one that has been a long time in the making.

This essay looks at significant milestones, judgments, policies, acts,
regulations, controversies, and cultural phenomena that have shaped and
signaled the rise and making of information overload. It takes pastiche
historicization to break the pattern of responses that accompany the
newness of digital media crises: the finding of new digital tools to
counteract the existing digital tools and pathologizing the user as the
corrupt variable responsible for these crises. When it comes to digital
media, the crisis of the now invariably looks for solutions in the
future, as if the immediacy of the crisis also precludes all
historicity. In this essay, by focusing on the infrastructural
production of overload and the informational shaping of the user, I
examine how we got to this state of information overload and the crisis
of the informational subject.

The first section tries to understand how the state of overload was
designed and naturalized through regulatory and policy frameworks on the
one hand, and the favoring of specific forms of informational behavior
on the other. The second section establishes that the user, who is often
seen as the agential unit of digital cybernetic feedback loops, is
compromised in agency and autonomy through the championing and the
perpetuation of informational overload. Both sections together present a
specific account of how we got to this point where information overload
is so ubiquitous and insidious that we do not even recognize it as a
critical condition and do not understand the materiality and historicity
of how it came about. Through an archive of milestones in India – some
popular, some lesser studied – this essay offers a way of reading some
of the most pernicious problems of our times as a result of the
engineering, cultural shaping, and political proliferation of the state
of overload.

## SECTION I 

## INFORMATIONALLY YOURS: OVERLOADED\

In July 2020, when India, and the rest of the world, was reeling from
the COVID-19 pandemic, a YouTube video went viral in the country.[^05NIshantShah_1]
Made by a self-described *YouTuber and influencer* Shubham Mishra, the
video shows Mishra sitting in his car, uttering profanities in unchaste
Hindi. He threatened a stand-up comedian Agrima Joshua with physical
violence and even rape. Mishra, a staunch nationalist and a moderately
popular influencer, who regularly created offensive and threatening
videos in the name of *calling truth* for his roughly 2.5 million
followers, was aggravated by a stand-up routine or set that Joshua had
performed a year earlier in 2019. In that set, Joshua mocked the Indian
government’s plans to build a massive statue of the beloved nationalist
icon Chhatrapati Shivaji Maharaj off Mumbai’s shoreline and the way
people on the crowdsourcing platform Quora were exaggerating and
embellishing the features of the proposed statue – laser eyes to kill
terrorists, solar cells to power the entire state of Maharashtra, and
GPS trackers to identify enemies, for example.

In his three-minute video, like a professional social media berserker,
Mishra managed to at once profess his love and loyalty for Shivaji,
appoint himself as the vanguard of all communities aggrieved by this
attack on their religious/nationalist leader, and call for the
cancelling of such *progressive* stand-up comedies. Announcing his
respect for women, he went on to lambast Joshua, call her the ‘N’ word
(coincidentally doing it while Black Lives Matter was taking global
anchor in its second uprising), and casually threaten her with rape and
death while inviting his followers to join him in teaching Joshua a
lesson.

The video, which has since been deleted and
re-uploaded multiple times, went viral amidst polarized responses from
those supporting Mishra and those appalled at the blatant display of
toxic masculinity and aggression woefully naturalized in the
entertainment-hate complex of digital social media. The renewed interest
in this video a year later led to a series of quick attacks on Joshua in
a manner that has become all-too-familiar by now. The ruling right-wing
party in Maharashtra unleashed an army of trolls who started threatening
and intimidating Joshua for insulting their beloved warrior king
Shivaji. Pratap Sarnaik, a member of the Maharashtra legislative
assembly representing the nationalist party Shiv Sena, wrote to the home
minister of Maharashtra Anil Deshmukh to prosecute the comedian for
making contemptuous comments against Shivaji.[^05NIshantShah_2] Deshmukh himself
tweeted to his affronted populace that he had ‘instructed CP
(Commissioner of Police) Mumbai and IG (Inspector General) Cyber to take
legal action expeditiously’, and urged everybody to ‘maintain calm’ and
let the law take its course.[^05NIshantShah_3] Caught in the storm, Joshua tweeted an
apology video and took down the offending recording, even as angry mobs
echoed Mishra that Joshua should be taught a lesson. They ignored her
apology, ransacked the café where the comedy sketch had been hosted, and
committed multiple acts of vandalism and destruction.[^05NIshantShah_4]

This narrative of digital bullying, intimidation, harassment, threat,
violence, and incitement to sexual assault that Joshua experienced – in
an intense viral cycle – follows an all-too-familiar trope of women’s
experiences on digital social media in India. It exemplifies the 2018
Thomson Reuters Foundation report that pegged India as one of the most
dangerous countries for women[^05NIshantShah_5] and the 2020 Global Press Freedom
Index which ranked it 142 out of 180 countries for erasure of the right
to free speech.[^05NIshantShah_6]

The Joshua–Mishra story might well have ended here, except for an
unexpected narrative twist. Outraged by Mishra’s particular brand of
misogyny, a growing movement began protesting the impunity with which
Mishra, together with a slew of hate-spewing influencers, was apparently
able to continue unaffected. Following a public outcry, the National
Commission for Women (NCW) took cognizance of the case,[^05NIshantShah_7] and Mishra
was arrested by Vadodara Police on charges of obscenity, outraging the
modesty of a woman, provocation to break public peace, public mischief,
and criminal intimidation. Police also seized his phone and booked him
under the Information Technology Act, 2000, for publishing and
transmitting lascivious material.

The Joshua–Mishra story is unremarkable in how such incidents have
become normalized on the Indian social web. However, some responses were
significant. Many of those responsible for joining the social media
outrage that had led to Mishra’s arrest were surprised, alarmed, and
angry, but also genuinely shocked to see their favorite video-sharing
platform hosting such content. Several were apparently discovering such
content for the first time on the web. There were heated discussions on
the YouTube video, on Quora and on Reddit, about how this might be
*scripted content*, a *publicity stunt*; how this amount of hatred is
*not natural*. People were busy examining Mishra’s previous videos (his
followers and video views increased tenfold in the process) trying to
present this instance as anomalous, hinting at a larger conspiracy
theory of who might have *put Mishra up to the job* or what might have
*triggered him* to use such language. It seemed impossible to view
casual misogyny as the naturalized state of digital entertainment in
influencer cultures. There appeared an overwhelming felt need to find
deeper, somehow more profound, explanations alongside hidden
conspiracies that might have made this video at all possible.

Mishra’s supporters, too, were analyzing his loathsome diatribe, seeking
to prove that, while admittedly violent and hateful, it did not either
explicitly or conclusively threaten Joshua with rape. Progressive
liberals were, they claimed, twisting Mishra’s profanity-laden words and
misinterpreting them because he was standing up for national honor. In
his own apology video, which he was forced to make after being
threatened with legal action, Mishra reiterated that he had not attacked
Joshua, but rather defended the honor of his people. He was indeed the
*victim* here, of conspiracy, of being baited by the secular press and
media to suit their own agendas of nation-hating.

For Mishra’s supporters and critics, this in-your-face video and its
production and reception were deemed suspicious. Debates arose on both
sides on what, to use Kellyanne Conway’s phrase, the ‘alternative
facts’[^05NIshantShah_8] were, how the video could not be taken at face value, and how
it didn’t necessarily mean what it said.

I point here to the ease with which both outraged factions slipped into
the examination of conspiracy, unable to see the obvious. Together with
the large amount of information generated around this entire phenomenon,
a critical new mode of subjectivity was created: that of being
*informationally overloaded*. I propose that the natural propensity to
take almost *any* digital content and expect something *more*, something
*hidden*, something *extra* than what meets the eye or is available on
the surface indicates a particular condition where blame,
responsibility, and culpability become ever negotiated and constantly
oscillating values without fixity. []The
production of Joshua, as a perpetrator who had to apologize for her
acts, and the presentation of Mishra, as a victim being wronged by angry
Twitterati, are not exceptional, but rather a regular occurrence in an
ever-expanding zone of crises produced by social media.

Information overload produces, then, a state of permanent crisis – one
that allows for inversions and suspensions, and allows for fixed values
to come *unstuck*, enabled by a set of paradoxes that frame our debates
around contemporary digital social media. In an attempt to unravel these
paradoxes, this essay provides a symptomatic reading of three major
*digital crises* that have emerged in India (along with the rest of the
world) over the last two decades that saw an astonishing democratization
of digital technologies even as it saw the digitization of democracy.
Building through these crises, the sections in
this essay help us understand the implications of a naturalized
information overload, and how such a condition allows us to unpack the
almost paralyzed debates on misinformation, verification, fake news, and
post-truth – contexts that destabilize nearly all our conversations on
political governance, from climate to social justice.

## 

## The Information (Overload) Crisis

I begin this section with a fundamental proposition: In the last few
years, without us even realizing it, and in an almost nondramatic
fashion, we have foundationally changed our idea of who we are as
information subjects. We increasingly define information as a
*condition* of our existence, a condition of naming ourselves and each
other. Our informational condition is now also what defines our
authorship, which in turn defines who speaks, on behalf of whom, in what
voice, and with what authority. Our questions of agency, choice,
freedom, and truth are all tied to conditions of informationality.

It appears also to be widely accepted that
such a condition of informationality exists at three different levels,
in the three kinds of relationships we have with information. First, as
subjects *of* information, where we don’t need personal and social media
or smart and quantifying devices to tell us that, primarily, when we
talk about anything at all, we talk about ourselves. A large part of our
everyday life is spent in producing information about who we are, what
we do, and how we relate to the world around us. It is not a surprise
that, with the rise of easy-to-access digital devices, we built *social
media* which built something that was not *new* as much as it was a
documentation of our authoring of our selfhood immersed within this
informational condition.

At a second level, informationality also makes us subject *to*
information, as it shapes our informational realities and the contexts
we live in. Our identities, subjectivities, opinions, choices, tastes,
preferences, and desires are continuously designed and influenced by a
variety of other information hubs. This is at the heart of communication
and marketing, and this is also the hold, in analogue media, of
advertising and propaganda. It is, then, not surprising that with the
rise of algorithmic data mining practices we
are being written more and more into information structures that
*determine various markers of who we think we are*. An emerging global concern of invisible data being stored and
circulated and manipulated and reintroduced into our lives is
essentially a recognition that we are more written against than writing.

Third, and perhaps most significantly, informationality also makes us
*subjective to* information. Not only do we produce habits of filtering
information that do not directly pertain to us, we also have a *positive
bias* toward information that is relatable, accessible, and customized
to our specific needs. These needs box us into filter bubbles in digital
networks, and explain why so many of our conversations are in echo
chambers of network neighborhoods that protect us from people who are
unlike us. We are aware that information can be excessive, intense, and
paralyzing, and hence we have learned to selectively filter out the
streams that can sustain our modes of being. This relationship is
historical as well as ingrained in our daily practices of life, labor,
language, and love. And because this relationship is so central to our
very biological, social, political, and emotional survival, we have
guarded it fiercely across history.

One of the ways in which we have protected ourselves within such a
relentless informationality is by thinking about what constitutes a
reasonable amount of information for a human person to process, analyze,
and execute. Hence, the much used, abused, and sometimes dismissed
notion of information overload. The moment you read this phrase, I know
you have a reaction to it – you are either rolling your eyes, shaking
your head in empathy, or bookmarking it for later because right now you
have a dozen other tabs open that are competing for your attention.

While information overload has been talked about extensively in the last
decade or so, connected with precarious labor in attention economies,
with scattered and fragmented lives shaped by \#FOMO (Fear of Missing
Out), and data circulation in digital networks, it is good to remember
that it is not as contemporary a concern as it appears. Ann Blair, a
historian of information, points out that in the Judeo-Christian
traditions of the West, the concern around *too many books* surfaced as
early as in the 1st century, where in Ecclesiastes we are cautioned:
‘But beyond these, my son, be warned: there is no end to the making of many books, and much study wearies the body.’[^05NIshantShah_9] This warning was
echoed in moral philosophy where Seneca, in his *Treatises*, mourned the
dangers of abundant information:

> Even for studies, where expenditure is most honorable, it is
> justifiable only so long as it is kept within bounds. What is the use
> of having countless books and libraries, whose titles their owners can
> scarcely read through in a whole lifetime? The learner is, not
> instructed, but burdened by the mass of them, and it is much better to
> surrender yourself to a few authors than to wander through many.[^05NIshantShah_10]

Similarly, even at the height of the accumulation craze in the mythical
libraries of Alexandria in Egypt, the concern about *How much is too
much?* was very much alive. As Kathleen Fitzpatrick shows in her history
of information design, *Planned Obsolescence*, it was the idea that too
much unfiltered exposure to information might paralyze the reader that
gave rise to librarians as custodians and keepers of the keys rather
than as access points and facilitators of knowledge.[^05NIshantShah_11] Mark Rose
talks about scribal cultures in the 5th and 6th centuries <span
style="font-variant:small-caps;">a.d.</span> in England, where the
Church educated young men to take the sacred duty of copying the Holy
Writ for circulation across the land.[^05NIshantShah_12] However, not just anybody
could become a scribe. Apart from privileges of birth and gender, the
to-be-scribe also needed to show moral fortitude and the capacity to
deal with the excessive information he would be exposed to in the course
of his literary education. Even when scribes were finally granted an
epiphany and tasked with the holy book, they were kept in cloistered
isolation so as not to transmit any possible madness that may emerge
from excessive sensory and information overload.

At the turn of the 16th century, with moveable type democratizing
information access, the concern around information overload took on a
more gendered tone. As Virgina Woolf, in her 1929 novel *A Room of One’s
Own*, reminds us, in her commemoration of Aphra Behn, ‘the blue stocking
with an itch for scribbling’, the trope of too much information leading
to more depravity was also used as a justification for keeping women
from reading or writing literature.[^05NIshantShah_13] Kate Millet, in her seminal
1970 thesis *Sexual Politics*, reminds us that there was a systemic
relationship between madness and reading, where the woman was considered
too fragile to deal with the cerebral processing that came as a part of
too much exposure to information.[^05NIshantShah_14]

In the late 19th century, the arrival of mass communication, especially
the telephone, opened the door to anyone being able to call to give you
information – spammers, cold callers, wrong numbers, heretics with
dubious content, strangers with predatory intent –putting us all in the
continued danger of lapsing into a state of excessive information that
would leave us uncertain about our own identities. The anxiety at this
point was not about our *ability* to discern credible information from
lies, but about an *instability in our sense of self* and its
well-being.

So strong was this idea of sudden bursts of information overload as
harming the self, that it even translated into facetious-sounding but
earnestly written editorials vilifying and demonizing the technology
apparatus itself. Carolyn Marvyn, a historian of information
technologies, in her book *When Old Technologies Were New*, fishes out
an editorial from *The Electrical Review* that relates the story of an
affluent Chicago woman who was looking for a housekeeper she could
entrust her children to while she traveled for a family emergency. A
housekeeper who was tending to another house recovering from scarlet
fever was recommended. As Marvyn recounts, ‘…she was urged to expedite
arrangements by telephone’. At first, she was ‘aghast at the
proposition, and was sure there would be great danger of infection’ by
wire, her fears a metaphor for all the elements of the world beyond
domestic control. After weighing the arguments of a knowledgeable
friend, she concluded:

> Well, I suppose I must risk it. I’ll have a servant call up the house
> and tell them be sure that the housekeeper changes her clothes and the
> sick children aren’t in the room where the telephone is; then I may
> feel justified under the circumstances in talking with her.

In 1894, *Electrical World* had reported that the ‘editor of a prominent
Philadelphia daily newspaper had cautioned his readers not to converse
by phone with ill persons for fear of contracting contagious
diseases.’[^05NIshantShah_15]

This idea of information overload – either caused by unexpected
information, excessive information, shocking information, dangerous
information, or misinformation, all resulting from what Alvin Toffler
described in 1970 in his famous *Future Shock* as ‘an information
explosion’ – has been naturalized and brooded over.[^05NIshantShah_16] For those of us
old enough to have lived through the turn of the millennium, we still
have memories of the Y2K scare where the entire world order was going to
collapse, as time-counting mechanisms of modern-day computers, unable to
grasp the millennial turn, would throw us into an information chaos. The
crisis-that-never-happened was perhaps best enshrined in the iconic Nike
advertisement that showed a jogger ‘just doing it’ in a world slowly
deteriorating into Y2K anarchy.[^05NIshantShah_17] The biggest worry about Y2K was not
just that infrastructure would crash – satellites collapsing, planes
crashing, banks coming to a halt – but a sudden savagery that we would
all regress into because the machines that regulated our information
consumption would give up on their task of information regulation.

I give you this huge, ahistorical, symptomatic
overview to go back to my proposition – that somewhere, in the first two
decades of the 21st century, our relationship with information shifted,
and that this shift is perhaps best characterized by information
overload. This is not, as my sketchy history shows, a new phenomenon –
every technology that sought to expand knowledge documentation and
information production has triggered worries about who we will become.
In short, we have always worried about what happens when we lapse into
information overload.

What is, however, *new* for the digital turn that we live in is that we
may have, perhaps for the first time in history, *stopped worrying about
information overload*. Or, to make it clearer, we no longer think about
information overload as the exceptional moment when we get bombarded
with too much information. Such a condition is not evidenced through
isolated incidents, nor does it require special coping mechanisms and
skills to deal with it and escape it. Our overflowing inboxes, our
continuous stream of notifications, the smart devices we see, and the
smarter devices that are invisible but watch us, have all created a new
informational subject – a subject whose *ontology* lies in information
overload. This has become our naturalized state of being, not a
futuristic phenomenon or a momentary condition to be separated and dealt
with in isolation.

If the older information subject was a subject worried about information
overload and how to escape it, the new information subject understands
itself *through* the condition of incessant information. This is why we
participate in the continuous mining of data by devices that give us
beautiful visualizations masquerading as profound self-knowledge and
truth. It is perhaps why we subject ourselves to the ever-expanding
field of algorithmic surveillance that gives us convenience for privacy.
This may also be why we see ourselves as willfully participating in
polarized positions that are neither illustrative nor reflective of our
subjective selves, but performative of the networked mechanics that
shape the digital.

I seek to establish this naturalization of information overload as a
*condition* of crisis, which not only engineers and perpetuates the
contemporary crises around automation, fake news, algorithmic
polarization, and filter bubbles, but is a *crisis in itself*, which
needs to be unpacked beyond the questions of usage, penetration,
regulation, and control that are often addressed when dealing with
information overload.

## 

## The Information Overload (Continual) Crisis 

The information overload crisis, as in the case of the Joshua–Mishra
controversy, can be studied – and has indeed been mainly analyzed – at
the level of content: in editorials, analyses and conversations about
the nature of hate speech and the polarizing formats of digital
engagement. However, such analyses do not
always help us understand the phenomenon where *both* Mishra and Joshua
are presented as victims, and how *all* the narratives seem to be
received as potentially fake and are thus at once informationally
overwhelming as well as deficient in credibility. This particular condition of information overload as both
*saturated* and *depleted* by informationality has to be understood as
*itself* a crisis as opposed to the *reason* for a crisis. It
establishes information overload not as a future horizon or a historical
event but a *specific condition of the digital* perpetuating and
generating itself as a continuous condition.

In technology studies, and particularly in critical code and software
studies, such a crisis has received a lot of attention. The French media
philosopher Bruno Latour proposed the idea of ‘reversible black-boxing’,
where he takes the example of a broken overhead projector to propose
that, upon breakdown, a technological object transforms from being an
enclosed object to a network of different agents – actants – making up
the performance or idea of the object.[^05NIshantShah_18] This framework has found
much traction because it recognizes the breakdown as a state of crisis
where the user’s attention is directed from the system as an enclosed
object to an awareness that the system is constituted by different
parts.

It is possible to take the Joshua–Mishra case as an instance where the
expected smoothness of social media gets interrupted by the intrusion of
the legal apparatus that seeks to regulate the content and its
distribution. It is important to notice that the Terms of Service of
digital services and the Information Technology (Intermediary Guidelines
and Digital Media Ethics Code) Rules, 2021 were ineffective in actually
curbing the spread and the visibility of this hate speech. Despite
multiple people reporting the videos and the call for action, neither
Instagram nor YouTube took responsibility for censoring and removing
such content. Even as people were commenting on the broken nature of
this social media engagement, amplified by discussions in traditional
media, it was clear that multiple agents were needed to make sense of
this digital crisis. The swift intervention of the law, the arrest of
Mishra, the stepping in of different celebrities and politicians, the
response from the political party that sought to investigate Joshua as
also culpable, all suddenly make themselves visible in the
infrastructure of otherwise opaquely transparent interfaces of our
digital devices where these crises play out.

American sociologist Susan Leigh Star builds on Latour’s work in her
‘call to study boring things’.[^05NIshantShah_19] She seeks to understand crisis as
located in neglected and invisible systems – not systems that work and
might break, but systems that have long stopped working, but are still
around, forming a massive infrastructure of what German media theorist
Wolfgang Ernst calls ‘undead media’.[^05NIshantShah_20] While Star was particularly
interested in thinking of ‘computers as information highways’ and ‘as
symbolic sewers’ to open up the back ends of global information flows,
which otherwise remain ‘buried in inaccessible electronic code’, Ernst,
by contrast, was identifying the crisis in the time criticality of
digital computational media which converts the computer into ‘a complex
time machine’ and manifests it as ‘equiprimordial’ (temporally
undistinguishable).[^05NIshantShah_21] Media do not have a historical past, says
Ernst: while in operation, they exist outside of historic time in a
state of micro-temporality, a synthesis of the past and the present in
the now.

New Media theorists Hertz Garnett and Jussi Parikka draw on Ernst’s idea
of crisis as constituted in the very operation of media, and propose
that *all media is always in a state of degeneration* and hence always
on the precipice of obsolescence – a state of continual crisis. Hertz
and Parikka present ‘planned obsolescence’ as a crisis horizon that can
both be bent and differed as art and design practices ‘resurrect,
reanimate, and reappropriate’ discarded dead media, turning them into
new assemblies of ‘zombie media’.[^05NIshantShah_22] This resonates with Lauren
Berlant, who, in their exposition on *Cruel Optimism*, reminds us that
the naturalization of crises of ‘life-building’ has so overwhelmed our
experience of living that ‘adjustment seems like an
accomplishment’.[^05NIshantShah_23] Critical Code studies theorist Wendy Chun begins
her book by claiming that ‘new media exist at the bleeding edge of
obsolescence’, thus necessitating a continued state of ‘updating to
remain the same’.[^05NIshantShah_24] Chun argues that the ‘twinning of crisis and
code/habit’ (perhaps a perfect description of social media engagement)
‘has not diminished crises, but rather proliferated them through an
unending series of decisions and unforeseen consequences that undermine
the agency they promise’.[^05NIshantShah_25] In her characteristically pithy way, Chun
announces that in ‘new media, crisis has found its medium: and in
crisis, new media has found its value, its punctuating device’.[^05NIshantShah_26]

Information overload as a condition of crisis is itself critical because
it does not seek to dissolve itself or offer any resolutions for the
short-lived but intense moments of engagement it generates. Rather, it
normalizes a state of continual crisis, manifested in different events
that rise and fade without the crisis ever going away. It is from this
sense of a crisis that we shall now try and make sense of how digital
discourse and practice have been shaped in India.

This essay lays out the story of the internet in India as a story of the
naturalizing of information overload. It tries to make sense of current
debates around disinformation, fake news, post-truth, and governance as
structured and informed by such naturalization. In order to do this, it
establishes three paradoxes that mark the naturalization and anchors
these paradoxes in *social media crises* that have pockmarked the
history of the internet(s) in India. I hope to retell the story of
infrastructure, governance, regulation, and policy through the
often-overlooked questions of affective, libidinal, and lived
experiences of the people. This will hopefully also return our focus
back to the human actors who are often a part of the technological
crises, but are made invisible by the focus on the technological terrain
and the idea of the user as the predominant way of describing and
resolving these crises.


## Your Access/Accessing You

On August 25, 2015, the state government of Gujarat imposed an
unprecedented internet shutdown on the entire state.[^05NIshantShah_27] For the state,
which had built itself up as the poster child of digital development,
through reurbanization, opening up public sector projects for private
investments and offering tax breaks to information technology companies
to build their development-making centers in the state,[^05NIshantShah_28] this was an
unexpected and unprecedented move. The promise of economic and inclusive
growth, enshrined in the ruling party’s slogan *Sabka saath, sabka
vikas* (Everyone’s support, everyone’s development), and the image of
Gujarat as a new IT state that was investing in the digital future, made
this an unexpected site for a shutdown. While other parts in the country
have a continued history of internet shutdowns, these were generally in
states that saw conflict, where the suspension of digital access and
civil liberties appeared necessary for security and sovereignty.[^05NIshantShah_29]

The movement that ushered Gujarat into this state of digital emergency
has a 22-year-old politician, Hardik Patel, as its poster child. The
convener of the Patidar Anamat Andolan Samiti (PAAS), a political body
that advocates for minority rights for the Patidar community[^05NIshantShah_30] by
including them in the category of Other Backward Class (OBC),[^05NIshantShah_31] Patel
has been actively involved in organizing massive rallies since the
summer of 2015.[^05NIshantShah_32] The on-the-ground rallies have also been
accompanied by a popular social media campaign which included YouTube
videos, messages, memes, and even two Android apps that mobilized the
community through weak people-to-people networks.

It took a small but dedicated core team of young political leaders to
put together the *Maha Kranti* Rally (The Epic Revolution Rally) that
engulfed the whole state.[^05NIshantShah_33] While each major city in Gujarat was
organizing the coordinated demonstration, the biggest protest gathering
was planned in Ahmedabad. Beginning at the massive Gujarat Mineral
Development Corporation (GMDC) ground, the rally attracted more than
half a million members of the community, who, after some political
speeches, held the city under siege, marching to the district
collector’s office. Patel, who was one of the speakers rallying up the
crowds, announced that at the end of the rally he would go on an
indeterminate hunger strike until the chief minister of the state
herself came to receive the memorandum. Both he and his immediate allies
were arrested for not having the adequate permissions to stay on the
ground after the rally. Although they were later released, tensions had
already escalated and the city saw the deployment of police and
paramilitary forces to disperse the agitating crowds that were already
demonstrating acts of mob violence. Ten people (police and protestors)
would eventually be killed across the state. The state government of
Gujarat imposed physical curfews in a few cities along with a complete
shutdown of the internet.[^05NIshantShah_34]

The story of PAAS, and the electoral and political results it led to,
merits another analysis. Here it is important to place its role in the
blocking of internet access, and thus to tell a somewhat different story
of the promise of access that has shaped the history of the internet in
the country.

*Access* has been one of the primary drivers of digitalization and
investment in internet infrastructure that was supposed to leapfrog the
country into a digital revolution. In fact, the very first definition in
the Information Technology Act, 2000 is for the term access.

*Access* with its grammatical variations and cognate expressions means
gaining entry into, instructing, or communicating, with the logical,
arithmetical, or memory function resources of a computer, computer
system, or computer network.[^05NIshantShah_35]

Even in this definition, both the conditions and means of access are
clearly nuanced. Access was not just about usage *but any meaningful
interaction with the entire digital ecosystem*. Access was discussed in the context of unauthorized access;
storage, retention, and retrieval; licensing and public access;
availability and perpetuity; security and legitimation; denial and
maleficent blockage of access; privacy and replication of information;
and markers of public space.[^05NIshantShah_36] In that very first laying out of the
regulations of legal and acceptable forms of transactions in the digital
networks, access was clearly one of the most cited and critical clauses.

I have explained earlier that both the emphasis on access as well as the
conditions of its possibility are mirrored by Access to Technology (A2T)
developments across information societies in the face of digitalization.
Access has been fetishized as the aspired-to end of all technological
infrastructure, and also as the point of danger that allows for criminal
practices to proliferate. Access to technology remains central in IT4D
portfolios which look at universal access as the endgame. Government
practices recognize lack of access to digital infrastructure as an axis
of discrimination and seek to invest in creating access
opportunities.[^05NIshantShah_37]

Access, here, typically carries a double bind
of anxiety. On the one hand, it generates anxiety about the need to
*grant* access. On the other, and immediately afterward, it triggers
concerns about the *control and regulation* of practices that emerge.
Access-centered discourse overrides the complex terrain of the
human-technology relationship – usage, adoption, penetration,
internalization, proliferation, nudging – and becomes the single point
of obsession in telling the promise of the internet. More than 15 years
after India’s first Information Technology Act, 2000, we see this double
bind emerging, where we celebrate the participation of young people in
‘Digital India’ economies, while also shaping their behavior by banning
undesirable content. Likewise, in the world of user-generated content,
there is a celebration of the participatory cultural processes, of
peer-to-peer sharing and distribution, and of remixed and reused genres
that show the possibility of creative explosion in the age of ubiquitous
access. At the same time, there is a growing concern that these new
regimes of cut-and-paste creativity are leading to an explosion of
information that is being mined by predatory algorithms and data mining
practices that make the subjects extremely vulnerable. Access is often
thought of as a one-point entry into the digital world.

Yet, in the Information Technology Act, 2000, access was more a
*condition* than it was an interaction, and this becomes clearer in the
amendments made to the Act in the Information Technology (Amendment)
Act, 2008. On the one hand, access became closely tied to the
*infrastructure of access*, specifically looking at the emergence of
‘cyber cafes’, and examining the idea of ‘cyber security’.[^05NIshantShah_38] On the
other, there arose a particular focus on the extended practices of
information and data protection, ‘access, use, disclosure, disruption,
modification or destruction’,[^05NIshantShah_39] recognizing the threats that come
when ubiquitous access becomes the norm. The amendments mainly
concentrated on the potential for transgression that emerged with web
sociality. They thus envisaged the instituting of controllers with
extraordinary access to computers if ‘he has reasonable cause to
suspect’ that the computing network was used to break the laws set out
through this regulation.[^05NIshantShah_40]

The Information Technology (Amendment) Act, 2008 recognized the role of
service providers and those who would be made responsible for providing
access, and thus also for mitigating any threat that emerged out of the
access. What such recognition did was to enable the government to ‘issue
directions for interception or monitoring of information through any
computer resource’,[^05NIshantShah_41] that would eventually introduce the
intermediaries as critical to shaping the conditions of access.

On the one hand, the amendments to the Information Technology Act, 2000
clearly acknowledged the internet as a cultural force. On the other
hand, the amendments were primarily aimed at containing and regulating
the internet, focusing on usability and agency, because of its ability
to disrupt ‘public order’ and because it could ‘have \[a\] debilitating
impact on national security, economy, public health, or safety’,[^05NIshantShah_42]
all of which needed to be more monitored and contained for better
governance. The *promise* of access was not merely one public agency and
the right to informational technologies. It was also about training the
public into becoming responsible and responsive, and for training
regulatory units to meet the expectations of these emerging
technologies. The design and regulation of technology was thus
simultaneously the design and regulation of the *intended user*,
creating the need for devices that shall train the user to become the
technosocial subjects to now be shaped around the promise of access.

## 

## Meeting Technologies Halfway

Digitalization was aspirational, arriving before it was experienced and
heralded before it materialized. The accent of digitalization was
virtuality, but access to the digital went beyond infrastructure.
Infrastructure was a way by which the digital user was shaped to meet
technologies halfway. This was not in itself an unexpected or
unprecedented development. In the mid-1960s, in the United States, when
the first idea of mass digitalization was being floated, Douglas
Engelbart was examining, as a part of his ‘Augmenting Human Intellect’
project of 1962, how to develop ‘new techniques, procedures and systems’
to enhance the ‘effectiveness of the individual’s basic
information-handling capabilities in meeting the various needs of
society for problem solving in its most general sense’.[^05NIshantShah_43] Engelbart’s
ambitious and influential research came up with many conclusions and
recommendations for humans and computers to work together, some of which
pioneered and shaped the Graphical User Interface as we know it – from
the first prototypes of a Macintosh by Alan Kay to the production and
introduction of the mouse and the GIMP design.[^05NIshantShah_44] However, implicit in
almost all of the findings was the idea of information overload and
information processing.

In staging the problem, Engelbart proposed that ‘the entire effect of an
individual on the world stems essentially from what he can transmit to
the world through his limited motor channels.’ However, most problems we
directly grapple with do not rely on our motor skills but on innate
sensory inspection and cognitive capabilities. Engelbart thus broke
human capabilities down into four classes: Artifacts, Language,
Methodology, and Training, all of which could be stored as information
on computer-controlled systems, which could handle massive amounts of
information and display it when needed, thus enabling a transfer of
knowledge and the adaptive invoking of expertise whenever required for
dealing with new problems. Engelbart’s basic presumption was that the
human subject was not entirely capable of explicitly recognizing its
knowledge and, when confronted with it, not always able to discretely
and efficiently process that information or to make the most effective
decisions. The human, in Engelbart’s proposal, needed *augmentation* –
because we are continually paralyzed by the informational overload of
our own knowledge and perception. We needed an external augmentation
device that would offer prosthetic help to move from ineffective to
effective, and then to brilliantly effective, when faced with complex
problems. A *device*, then, which would, as it were, allow us to access
our own information through intelligent filters, so that we can cope
with massive information without being overwhelmed by it.

The production of the device was not a straightforward digitization
process of deep mining the human subject for information, however,
though that would become one of the logical fallouts of the process.
Engelbart’s own efforts were directed at training machines that can
learn from human actions and interventions and, subsequently, training
humans to be able to work with these machines. Augmentation was not just
an *expansion* but also a *reworking* of human capabilities to work with
these machines of expansion. So much so that he wrote a tutorial on
‘Games that teach the fundamentals of computer operation’, where he
devised a way by which ‘a group of common laymen’ might be taught how to
‘coax sophisticated information-handling behavior from an organization
of simple physical elements \[…\] to simulate various kinds of simple
elements by organizing them into a network’ whose behavior is obviously
more sophisticated than that of any single element.[^05NIshantShah_45] Whimsical,
overly elaborate, and long-drawn as this game might be, it does signal a
pivotal moment in technology design, where it became clear that for the
*human* to be effective, new information-overload managing technologies
would be necessary. Subsequently, for these *technologies* to be
effective, the human user would have to be trained to meet technology
halfway.

In India, some of the earliest manifestations of this human-technology
access and co-design was in the problem of access that was both at the
level of infrastructure penetration and systemic unevenness. Through the
1990s, as the first wave of telephone connectivity was receding, the
infrastructure of telephony was quickly transformed to become the basic
infrastructure for all digital access. People close to the ground, and
especially those who were aware of the back-end costs of digitalization,
had already recognized that reducing access to infrastructure would not
be enough. While it remained necessary to develop the digital ecosystem,
a promise of mass connectivity would need more: the design of a user who
could indeed be installed and meet the technology development demands.

Hence, Simputer, the first ‘local’ computing machine, proposed in 1998
as a ‘low-cost, usable and *useful* and usable to the common man’
solution. The Simputer group, located at the Indian Institute of Science
in Bangalore, under the leadership of Vijay Chandru, found that the
‘high cost of initial acquisition’ made computation unimaginable for the
average person. They also recognized that the ‘equally high cost of
maintenance and upgrade’, and the ‘complete lack of user-friendly
interface’ combined with English as an alien language to a vast majority
in the country, made computation prohibitive.[^05NIshantShah_46]

The Simputer – A ‘Simple Computer’ or, as its co-inventor Swami Manohar
writes, ‘if you prefer ridiculously complex recursive acronyms,
‘simputer stands for Simple, In-expensive Multi-lingual PeopLE’s :-)
comPUTER’.[^05NIshantShah_47] (I am as intrigued by this proposition of access for and
by the people as I am by the fact that back then, before the world of
emojis had exploded on us, Manohar was already using smileys in his
documenting of the project.) The Simputer was a prototype to be designed
and commercially sold for under Rs 5,000 to address three key
application areas: ‘Transactions, Communication, and Information’. It
was supposed to facilitate economic transactions, establish
interpersonal communication, and give access to the ‘right information
at the right time’ so as to ‘dramatically improve the quality of life at
all levels’,[^05NIshantShah_48] including in political and electoral participation and
governance.

The ‘make or break’ point of the Simputer was its interface. Or, as the
pithy statement said, ‘The simputer IS the user-interface’. For a
country struggling with massive illiteracy, functional literacy, and
semi-literacy problems across multiple languages, a text-based interface
was exclusionary and thus futile. The Simputer group was already zeroing
down on voice inputs and speech outputs in local languages, ‘augmented
by a minimal single-line display’ as the best, indeed the only feasible,
option. Combined with a user interface of the TV remote, this produced
the Simputer as a phone with a keyboard – a concept that sounds quaint
from the contemporary vantage point, but was revolutionary for that
time.[^05NIshantShah_49]

The Simputer, in recognizing the need for a visually dense interface,
telephonic connectivity, and simple navigation structures, was already
preempting the mobile revolution that leapfrogged India into the
information age. However, the really visionary qualities of the
Simputer, even though it never quite made it in the mass market, was its
way of predicting three critical points in access infrastructure that
bring us back to the phenomena of access and disconnectivity we
discussed earlier.

The scientists behind the Simputer indicated that the infrastructure of
the future would be dynamic. Devices with their own IP address, new
protocols to manage these dynamic IP addresses, and hyperconnectivity
through Local Area Networks, all this would be part of the architecture
managing the projected massification of the internet. In the
standardization, they saw a clear hierarchy: the *application* was the
responsibility of the intermediary, but the *infrastructure* would be a
state enterprise. The state, in their vision, would hold ownership over
the entire ecosystem from spectrum to device in order to facilitate a
comprehensive suite of transactions. Standardization would necessitate
not only the setting up of technical protocols, but also a clear
guideline on what can and cannot be said. The state, then, would have to
evolve into the digital system to become the primary service provider
rather than just a regulator. The state must access the digital before
the user does.

However, before the state could access the digital, digital technologies
would have to first reach the physical user. With its emphasis on costs
and the chief ambition of being ‘inexpensive’, the Simputer advanced the
concept of ‘smart card’ access. The project underestimated the seduction
of private ownership and was thinking of communal access devices where
entire communities would use one Simputer to manage their needs. This
also led to the possibility of a cloud-based, modular infrastructure
where, based on the personalized login of a user, the same device could
become temporarily personal, thus allowing for multiple ownership and
usage paradigms: a structure that (we saw) had been implemented by
cybercafes in the late 1990s. With smart card access, the Simputer was
already proposing that individual voter identities, electronic cash
repositories, and other essential services be tied together to form a
specific profile which in turn served as an access point to the
expanding universe of computational information services. *The
individual would have to thus be first accessed by the digital
technologies before s/he could be authorized into the system*.

Lastly, the Simputer focused on questions of security. The Simputer
group recognized that putting this information, data, and ‘money’ on the
digital network came with additional risks of theft, loss of control,
and hacked leaks that put the individual in vulnerable conditions.
Security concerns were central, but were also a known risk that had to
be taken. The personal risk of security breaches is, the Simputer
suggested, negligible when compared to the far more real threat posed by
the digital transaction ecosystem to the regulatory state. The digital
transaction ecosystem identified anonymous transactions, unverified
identities, direct access communication between people, and undocumented
movement of information as posing serious challenges to the state, even
as the state reconfigured itself in digital space. The model of the
Simputer thus showed that both the digital infrastructure and the
governance ecosystem access each other and share the information around
the user before the user could be given access to the promise of the
digital revolution.

It is telling that the Simputer had already embodied the basic paradox
of information overload and access. In order for the user to access the
information superhighway, the user needed to be *made accessible*, thus
meeting demands of both state and technology for legibility as well as
intelligibility. The user needed to be designed toward specific
protocols and behaviors in order for a digital transformation to emerge.
This also preempted the fact that universal access was not only about
building access infrastructure, but that it was also about leapfrogging
users into a techno-governmental regulation model that would shape them
to meet technologies halfway.

The Simputer was part engineering, part science fiction, part fantasy.
It imagined a future that we have now naturalized, even though it never
managed to be a significant player in that future. It preempted the
principles of the mobile revolution as well as the rapid transitions of
the Indian state toward ‘Digital India’ policies. However, more than
anything else, the Simputer presented the first model of a *user who
would be designed and regulated*, both because of the user being
information-hungry as well as informationally overloaded, alongside a
new model of a fresh state-technology nexus to sift credible data from
the mass of unnecessary information that surrounded it.

## 

## What You Get is What You Want

The digital turn, we have argued, was aspirational. We may take this a
step further and argue that the aspiration was for transgression. The
dramatic changes in media practices that the digital ushered in have
found many terms – convergent media, post-media, disruptive media,
collaborative media, connected media. Regardless of the name, what marks
digital media practices is their power of transgression, the capacity to
not just blur boundaries but produce an aggressive disregard for any
boundaries at all. The promise of access as unleashing *unbounded*
access in turn produced unbound practices that became central to how we
saw the potentials of access.

It is impossible to make an exhaustive list of all the transgressions
that digital media practices enabled. Perhaps the most evocative
embodiment could be the playful and provocatively fictive ‘Rule 34’ meme
that asserts that ‘If something exists, there is \[internet\] porn of
it’. In memetic truth, Rule 34 shows how nothing is off limits when it
comes to the internet, and how *accessing the web is to already be in a
state of transgression*. It is easiest to understand the state of both
transgression and access by looking at the burgeoning phenomenon of net
porn. This is particularly true in India, which has the dubious honor of
being among the top ten consumers of online pornography ever since
MMS-enabled phones became popular.

India’s regulatory authorities have used every technological means at
their disposal to try to stop the libidinal flow of pornographic content
– from censorship to blocked ISPs, from individual penalization to
intermediary liability. The fact that nothing has worked leads me to
build a Rule 35: ‘If there is porn on the net, people will access it’.
Transgression here means not just accessing the forbidden or blurring
the boundaries. The potential of transgressive access is in its capacity
to engineer desire, expression, and freedom. I propose that
transgressive access needs to be understood as more than an adolescent
glee of breaking rules. Transgressive access needs to be understood as a
question of agency within the logics of regulation.

It is perhaps possible to tell the story of access as transgression
bookended between two of the largest public debates around
transgression, morality, and pornography in India. The first instance of
transgressive access that is also the most popular (but forgotten in
viral time) is the story of the famous DPS MMS (Delhi Public School
Multimedia Messaging Service) case. A short, grainy, low-resolution clip
that portrayed two students, allegedly studying at the Delhi Public
School in New Delhi, went viral in 2004. The
pornographic clip, shot by the male student using his then cutting-edge
camera-enabled phone, shows the face of the female student performing
fellatio as he verbally encouraged her. The male student leaked the
video after he was dumped, and the clip first made its appearance on the
closed networks of the school. In the friend-of-a-friend logic of
digital networks, the clip soon went viral on the internet. Arguably one
of the first instances of user-generated pornography in India, it became
the origin point of the spy cam – the hidden cam, the POV, and other
invasive and amateur pornographic conventions – that has now been
mainstreamed and naturalized across various porn tubes.

When the MMS first emerged on the digital landscape of smutty social
media in India, however, it took the country by storm. As Namita
Malhotra points out in her landmark monograph examining the
intersections of law, pleasure, video, and porn in India, everybody was
looking for it.[^05NIshantShah_50] On the pervert-2-pervert network, the need to see
‘real people’ with ‘bodies like ours’ having ‘Indian sex’ caught on
video resulted in spikes on search engines and local sharing networks.
The clip also found its way into grey markets where it was sold as ‘real
sex’ as opposed to ‘imported porn’.

Even as the clip went viral, an enterprising student, Ravi Raj, at the
Indian Institute of Technology Kharagpur decided to capitalize on the
demand and put the clip up for auction on the site Bazee.com where it
appeared as an e-book under the scintillating title *Item 27877408 – DPS
Gurl having fun!!! Full video + Bazee points*, priced at Rs 125. The
state regulatory apparatus, which had no way of recognizing and
controlling the ‘nether spaces of p2p networks’ or ‘covert exchanges on
mobile phones’ found its first engagement with this e-commerce
transaction and immediately banned the offensive clip.[^05NIshantShah_51]

A public interest litigation was filed against the clip and a bizarre
case ensued. Once all the bodies involved in the making and circulation
of the clip were identified, nobody could be found guilty. Ravi Raj was
taken into custody for possession and intention to sell pornographic
material but was acquitted in juvenile court and expelled from his
university. The male student who was the clip’s
producer/actor/distributor was also acquitted: his crime, according to
the presiding judge, was of being part of a society that is rapidly
being influenced by Western Culture. The female student, the most
visible body of the video, was recognized as a victim, and mechanisms
were set in place to protect her identity and her future. When none of
these three obviously responsible people could be charged with the
crime, the state sought a ‘symbolic resolution’ by extraditing the CEO
of Bazee.com, Avnish Bajaj, from the United States and recognizing him
as ‘the foreign-educated man who had been touched by the spread of such
sleaze’.[^05NIshantShah_52] Bajaj was an easy target: it was over his visible body
that the case was resolved, at least for the public. He won the case,
pitting ‘Terms of Service’ against the Information and Technology Act,
2000, and thus claimed innocence as an intermediary.[^05NIshantShah_53] We shall
discuss intermediary liability shortly, but it becomes interesting that
in this one case, where several transgressive bodies were identified,
and several more desiring bodies were accessing the transgressive
material, that the blame eventually was put on the technology itself.

The court judgment argued that the true culprit in this case were ‘the
gadgets…which make possible certain kinds of technological conditions’
that caused ‘…obscene material to be published’. The technology – which
offered access to a ‘listing which informed the potential buyer that
such a video clip that is pornographic can be procured for a price’ –
was what needs to be controlled and regulated.[^05NIshantShah_54] The ensuing
regulations addressed the control of access to technology. Those found
in possession of the clip could be fined and prosecuted. The police in
Mumbai were given the authority to carry out digital frisking of
people’s phones to check for the objectionable content.

The concern was not about obscenity or pornography, but about the fact
that the digital was now producing ways to sidestep the state’s
authorial positions, and producing mutable, transmittable, and
transferrable products which could be accessed without regulation or
control. *The crime was thus not individual but collective*. The
culprits were not the four people in the case, but an entire
technosocial country participating in the proliferation and consumption
of the video clip. Access, an avowed goal of the state’s ICT4D visions,
suddenly produced the *user as a potential criminal* whose crime was to
access the digital, making such an act in itself into an act of
transgression. The *potential* of transgression thus made access into a
point of regulation and control: making access into possibly the most
contested space of digital engagement, thereby also turning access into
infrastructure.

Transgressive access was not just about infrastructure *of* access. It
was also about memory making and archiving. Wendy Chun, in her work on
Software Studies, points out that ‘the role of the digital is to make
memory into storage’.[^05NIshantShah_55] Chun points out that, as physical
computation, memory refers to actual storage. The web’s infinite
capacity to remember is tied to its material capabilities of storage or
information and data. Obsolescence, corruption, and erasure of storage
can easily invalidate memories and remove entire archives without any
respite.

Transgressive access thus meant that the user, who only had spectatorial
memories so far, was also suddenly equipped with archival storage
mechanisms. To access something on the web was to also have the
potential for storing it. The personal archive for public distribution
directly challenged the state’s authority of being the custodian of
histories and guardian of memories.

This was evidenced in another famous case around pornography in India,
this time enshrined in the illustrated figure of ‘Savita Bhabhi’.
Created under the pseudonym of ‘Deshmukh’, India’s first adult comic
strip series on the now banned website SavitaBhabhi.com depicted the
fantasy-filled sexual adventures of a ‘housewife’ whose husband was
traveling a lot for work, which gave her space to engage in pornographic
encounters. The 51 stories published in this series destabilized the
home as the bearer of family values and foregrounded a woman’s agency
and sexual desire. They also made the site the 82nd most visited Indian
website in 2009.[^05NIshantShah_56]

In 2009, the Department of Telecommunications (DoT) issued an order
asking all internet service providers in the country to block access to
the website. The Controller of Certifying Authorities, N. Vijayaditya,
explained in an interview that even though there was no legal order for
the ban, they were proceeding with it as a sign of respect to the voices
of protest from conservative quarters in the country. [^05NIshantShah_57] While Puneet
Agarwal, a second-generation Indian entrepreneur in the United Kingdom,
eventually acknowledged responsibility for the website and even
spearheaded the ‘Save Our Savita Bhabhi’ campaign, he did not challenge
the ban, giving into ‘family pressures’.[^05NIshantShah_58] And, thus, Savita Bhabhi
disappeared from the Indian web.

In this case, however, when the official distribution channels got
pulled down, the users who had been accessing the series revealed that
they were not just accessing but also archiving all the content.
User-generated torrents of archives marked by obsession, animated by
passion, and sustained through personal libido began appearing on social
media. Savita Bhabhi was given a viral lease of life as pervert users
suddenly became defenders of free speech and protectors of the public
memory that was being erased by the regulatory authorities. While the
official platforms – the legal owners, the intermediary suppliers, and
regulated channels – could be regulated and controlled to make Savita
Bhabhi disappear, the personal archives emerging from access could not
be subjected to the protocols of the gatekeepers. The checkpoints and
chokepoints of regulation could control the conditions of circulation,
but not the potentials of access. Transgressive access in this case was
not just a retaliation against the state: it challenged the state’s
ability to write memories and histories. Access became more than
infrastructure – it became a point of archiving – and thus also
challenged older forms of regulation and control of information, much as
it did in the case of the DPS MMS.

This idea of information access as not just *enabling* transgression,
but being *itself* an act of transgression immediately dovetails into
the second paradox of information overload: the information you get and
the information you want. On the one hand, access to information is seen
as a right, a necessary condition for participation and inclusion in the
information (overload) networks. Indeed, because information overload is
also at the basis of the economic and regulatory models of digital
governance and development, it is important to have all users
‘accessible’ on the premise of ‘having access’. However, the peer-2-peer
and distributed modes of information dissemination and circulation mean
that this access, an act of transgression, is continually seen as a
state of crisis, where it is simultaneously both a right and a threat.

## 

## Informationally Overloaded Threats

The regulation of information – its access, ownership, proliferation,
and infrastructure – clearly imagines the rise of people participating
in information production spaces as a threat. In the hands of young
users, the access to and ownership of information became a way of
bypassing social conservatism, leading to the proliferation of sexual
desires and identities. When equipped with information infrastructure of
proliferation, engaged citizens turned into active protestors, as in the
case of the PAAS agitators. As computing became ubiquitous, every
household with an internet connection became a potential hub for piracy,
with pirate crackdowns becoming a major concern. Minority and marginal
voices found an amplified presence, sparked off by the tragic case of
Rohith Vemula and the discussions around caste-based
discrimination.[^05NIshantShah_59] Survivors and victims of violence found platforms
to voice their struggles, creating lists that called out systemic
perpetrators, as in the case of the \#LOSHA (List of Sexual Harassers in
Academia) and the \#metoo conversations. When used by populist
vigilantes, these platforms have mobilized lynch mobs that have hurt and
killed people through social participation. The democratization of the
internet might have started as a romantic invitation to participate in
the political economies of the digital transformation, but the
consequences and results of this engagement have been extremely
threatening to ways of life and living.

The history of internet regulation has largely been a series of often
futile attempts using older models of censorship, blackouts, and
control. Many foolhardy attempts at blocking domain names, installing
censoring mechanisms, banning content, removing platforms, and hiring
‘content clean-up crews’ litter the landscape of controlling and
curtailing access. As Ashish Rajadhyaksha points out in his landmark
monograph *The Last Cultural Mile*, there was an inherent conflict in
these mechanisms of control: the state was trying to introduce
regulatory mechanisms that were relics of a centralized information
model, on to a system that was necessarily peer-2-peer and distributed,
and hence had different mechanics.[^05NIshantShah_60]

As network engineer and theorist Duncan Watts points out in his ‘Small
World Phenomenon’, the digital network is essentially a collection of
small worlds – each world self-contained and self-referential, but with
an infinite capacity for expansion and replication.[^05NIshantShah_61]
The regulation of physical computation
networks like the internet needs a different mode of control, one which
sets up instances of governance for every ‘small world’ and yet has the
capacity to scale that to each world that is simultaneously generic and
unique. Martin Warnke, a system theorist and digital cultures critic,
along with Carmen Wedemeyer, explains this as a ‘scale-free’ event.[^05NIshantShah_62]
Warnke argues that because the physical computing networks do not work
through the law of medians, but of correlations – looking not at
averages but at unique relationships – it is impossible to govern the
space through regulations and laws which are limited in their scale,
scope, and speed. Our older forms of regulation were devised around the
law of silos, of contained, scaled, carefully demarcated spaces, whereas
the internet is necessarily a space of bleeding overlaps, circulating
traffics, and flows of information which cannot be regulated or
contained.

In the face of this networked logic, the state is often unable to be the
vanguard, arbitrator, or gatekeeper of informational control and access
regulation. As one of the peers – nodes in a flattened network – the
state can serve as a hub through which different nodes operate, but it
cannot be a central node from which this control emerges. Hence, it
became necessary to think of a new layer of *state-like organizations*
that would take up state-like functions in order to help deal with the
threat of access.

The need for such a layer was already visible in the DPS MMS case, where
the regulation was seen to be the responsibility of a platform like
Bazee.com. However, the Terms of Service of service providers like
Facebook, Google, or WhatsApp does not hold them accountable for the
nature of the content. The service provider – also known as intermediary
– consisting of telecommunication companies, platform providers,
application owners, device manufacturers – was generally imagined as
neutral, whereas the informational relationship and its content was a
direct relationship between the state and the citizen-user. In the face of new networked logics, through the
1990s it became quickly clear that the logic and logistics of the
central governmental regulation were not going to work. Especially with
the rise of hate speech, gendered violence, terrorist recruitment, fake
news, and personal attacks, it has become apparent that the intermediary
can no longer be treated as a neutral delivery mechanism.

The Information Technology Act, 2000 (Section 79) provided
intermediaries with qualified immunity, where, as long as they follow
the prescribed due diligence and do not conspire, abet, or aid an
unlawful act, their lack of ‘actual knowledge’ of the content and the
failure to remove or disable access would not be punished. However, this
condition of what constitutes ‘actual knowledge’ has been quite vague.
Thus, in the intellectual property case of *MySpace Inc vs Super
Cassettes Industries Ltd.*, the Delhi High Court read the Information
Technology Act to declare that in the case of copyright infringement, a
court order is not required and an intermediary must act to remove the
content upon receiving knowledge of the infringement.[^05NIshantShah_63] Similarly, in
at least two cases of sexual abuse and lynching through mob violence,
the government has indicated that the responsibility of finding
solutions is on the service providers who, if they remain mute
spectators, would otherwise be ‘liable to be treated as abettors’ and
‘face legal action’.[^05NIshantShah_64]

In April 2011, the Government of India published Intermediary Guidelines
that prescribe, among other things, guidelines for takedowns by
intermediaries, thus controlling both the conditions of access and the
afterlife of access.[^05NIshantShah_65] These guidelines also mark the beginning of
post-access politics: control and contestation around access, not just
as the point of engagement but in the continued interaction with, and
ownership over, the content produced. Under these guidelines, *access
became conflated with agency*, where the intermediary was required to
‘publish rules and regulations, privacy policy, and user agreement for
access or usage of the intermediary’s computer resource’.[^05NIshantShah_66] The
control was not just about the moment of access, but about what happened
once access had been established: control, in short, also of *usage*.
Similarly, the intermediary was to be held responsible not only for
monitoring the access to its ‘computer resources’, but also the control
over ‘removal of access to any information, data, or communication link
by an intermediary after such information, data, or communication link
comes to the actual knowledge of a person authorized by the
intermediary’.[^05NIshantShah_67] The guidelines help us
understand that access is a two-way link; they monitor not only persons
accessing but also persons being accessed. They identify persons who
might be violated because of *other* persons accessing *their* data and
information and recognize the need for grievance redress mechanisms as
part of access regulations.[^05NIshantShah_68]

Such an extension of access control into the afterlife of engagement has
had some alarming consequences. The Centre for Internet & Society in
Bangalore, along with the Google Foundation, carried out investigative
research with a small group of intermediaries to determine ‘whether the
criteria, procedure and safeguards for administration of the takedowns
as prescribed by the Rules lead to a chilling effect on online free
expression.’ Legally and informationally wrong takedown notices were
sent to the intermediaries.[^05NIshantShah_69] A large majority of them, without any
scrutiny, immediately removed the content, showing the possible
‘chilling effects’ on free speech that result from such intermediaries’
power.[^05NIshantShah_70] The question of the *threat* of access found its solution in
controlling the very *conditions* of access by putting the
responsibility on intermediaries and allowing their terms of service and
takedown powers to override what might be fundamental individual
freedoms. This, coupled with algorithmic detections and no-obligation
removal practices, produced a different notion of regulation: where any
action or expression was removed even *before* it was officially charged
with a criminal or objectionable intent, and which considered *all*
information – indeed considered the very act of access – as potentially
criminal.

## 

## Rightful Information Overload

This right to information has been both a utopian promise and a
fundamental prerequisite for information overload to become our
naturalized setting. This right began as a question of access and
infrastructure shaped as a problem of hardware penetration. However,
over the last 30 years, it has become a question of agency, identity,
and expression, so much so that universal access is no longer positioned
simply as *good to have* but as a fundamental right. Lack of access to
the net is discrimination, and a preventive mechanism for the
realization of one’s true self. So strong has been this idea of access
as *right* that it culminated in one of the largest cases of public
consultation and protest around internet policy in India.

It all began with Internet.org, a Facebook-led not-for-profit initiative
that seeks to build infrastructure and conditions of universal access.
Its mission states that ‘the more we connect, the better it gets’ and
sets out to overcome ‘issues of accessibility, affordability and
awareness’ in the hope that ‘one day everyone will be connected’. One of
the largest initiatives under the umbrella of Internet.org is a program
called Free Basics by Facebook, that ‘provides people with access to
useful services on their mobile phones in markets where internet access
may be less affordable’. The underlying condition of Free Basics is to
help improve the lives of people who are hitherto underconnected or
unconnected. Free Basics partners with mobile operators to build
universal access infrastructure that gives people ‘access to basic
websites for free’.

Mark Zuckerberg, the CEO of Facebook, has already been identified as
moving the company from being a ‘directory of information’ to a ‘social
network’ to a ‘core social infrastructure’ of connectivity.[^05NIshantShah_71] In his
2013 white paper, ‘Is Connectivity a Human Right?’, Zuckerberg announced
a new milestone for making internet access available to 5 billion new
users. Recognizing the infrastructure-deficit of emerging network
societies, his proposal was to expand access with a marginal role for
governments as regulators of the spectrum, and then give the
responsibility to secure the right of access to intermediaries:
technology and telecom industries. Free Basics thus actively sought
partnerships with telecommunication and ICT intermediaries in order to
build what they saw as universal access.

Given that the Indian government had already recognized access as a
critical pillar for development and growth, it did not come as a
surprise that Internet.org was welcomed to partner with the government
in its quest to create access across the country. The program was first
launched in 2015 as ‘Free Facebook’, with viral television and digital
media campaigns showing young millennials in India holding up notecards
saying, ‘I want free Internet’.[^05NIshantShah_72] Facebook proposed pairing up with
Reliance Telecommunications in order to offer free access to a selected
set of websites for all consumers of Reliance mobile phones.[^05NIshantShah_73]
Effectively, Free Basics was the gateway to users being connected for
the first time as it provided them with initial free access that also
tied them to the Reliance-Facebook nexus as the basic infrastructure for
their internet. In a country like India, where 350 million existing
users access the internet through mobile phone, and almost 70 percent of
its 1.2 billion population wait for future connectivity, this was a
massive market share. Universal access clearly became the philanthropic
strategy toward securing a loyal customer base of new users who were
being ‘platformed’ into this new ecosystem. The right to access was in
the service of expanding the consumer of private telecom ecosystems.

This blatant use of developmental rhetoric to create a future monopoly
was severely critiqued by technology advocates, technology
entrepreneurs, information activists, and internet researchers. Apart
from this transparent ploy of market expansion, however,
\#SaveTheInternet groups[^05NIshantShah_74] also challenged Facebook’s claims to
universal access as threatening two core principles of free and
democratic access in the country.[^05NIshantShah_75] They claimed that in its
‘walled-garden’ design, Facebook was creating a restrictive environment
and overriding the principles of zero rating and net neutrality in the
country. They started a massive campaign, pulling in one of the largest
comedy groups, All India Backchod (AIB), into their fold and a public
mobilization toward saving the internet and the principles of net
neutrality.[^05NIshantShah_76] They showed how Free Basics could create a
discriminatory preference of Facebook’s selected websites while
downgrading access to the others. They demonstrated how this building of
a *super* super information highway would throttle innovation and
smaller services, which might be disruptive to Facebook’s future
services. Through blogs, hashtags, twitter campaigns, comedy videos, and
print media, the \#SaveTheInternet group mobilized more than a million
emails and comments to TRAI[^05NIshantShah_77] in response to their consultation paper
on service differentiation in less than a month, asking for a free and
democratic internet that does not get compromised in the quest for
universal access.

The Cellular Operators Association of India launched their
countercampaign \#SabkaInternet (Everybody’s Internet)[^05NIshantShah_78] arguing that
Free Basics was a way to leapfrog the population into mobile
connectivity. Facebook responded to the \#SaveTheInternet email campaign
with its own public relations promotions of Internet.org as a ‘step
towards digital equality’. Massive banners, emotionally charged
advertisements promoting its mission to ‘connect the unconnected’,
became a part of its strategy.[^05NIshantShah_79] They created poster children – a
farmer who looks up weather information and commodity prices and learns
new framing techniques; a woman finding access to independence and a
break from patriarchy as she gains access to education through digital
services – in order to position themselves as the new saviors helping
unconnected India transform into Digital India. Mark Zuckerberg took
center stage, talking with Prime Minister Narendra Modi, visiting
different parts of the country to promote technology empowerment
dialogues, and reassuring everyone that while user growth was good for
Facebook, it was only part of a larger mission for a more diverse
ecosystem.

Despite its large advertising machinery, Facebook’s Free Basics lost
ground. The Telecom Regulatory Authority of India (TRAI) ruled against
its ‘differential pricing’ and ‘zero-rating plans’ which allowed mobile
phone companies to offer only a few services for free.[^05NIshantShah_80] The judgment
endorsed the position taken by the \#SaveTheInternet group that in a
yet-to-be-connected country like India, ‘allowing service providers to
define the nature of access would be equivalent to letting TSPs shape
the users’ internet experience.[^05NIshantShah_81] India became the first country that
fought against the lobbying powers of Facebook and retained the
conditions for free and democratic access, guarding these conditions as
critical to the fundamental rights of the citizen. Even with the
government’s flagship programs in ‘Make in India’ and ‘Digital India’,
which seek to create a ‘digitally empowered society and a knowledge
economy’,[^05NIshantShah_82] and despite its close association with tech giants like
Facebook, Microsoft and Google, it had to succumb to the idea of access
as such a fundamental right that it could not be transferred to
intermediaries. It also resulted in the \#SaveTheInternet group
launching themselves into the Internet Freedom Foundation, with an
explicit mission to ‘defend freedom of speech, privacy, and our digital
commons’ in the face of the new technosocial nationalism.[^05NIshantShah_83]

The story of Free Basics, its reversal and the continued promotion of
universal access, becomes a logical bookend to the story of the internet
itself as it got concretized in the Information Technology Act, 2000. It
offers an illustration of disconnectivity in far more insidious ways
than internet blackouts, regulation of content, or the moderation of
expression. It shows how companies benefitting the most from maintaining
information overload and all its paradoxes continue to exclude users who
participate as data subjects from the new data owners produced in these
conditions. The subject remains informationally circumscribed but also
severed from the regulation of the digital spaces and, ironically,
information overload remains the way by which this exclusion is
engineered.

## Stitching Things Together

At the core of the argument around information overload is an attempt to
historicize this as a crisis-in-the-making. While the history of
computation and the internet has often been written as a history of
information access, I have attempted to rewrite it as an history of
informationally overloaded access, showing how access is both a
multidirectional process as well as a transformative paradigm. It has
been well noted that production digitalization is not merely an
infrastructural project but a technosocial revolution that rewires
circuits and bodies, people and societies. Through the lens of
information overload, it becomes possible to see how the infrastructures
of digital spread move beyond the hardware of computational networks and
incite an entire ecosystem of regulatory, political, and technological
actors to create conditions of informationality that anticipate and
write a subject to occupy it.

Or, in other words, the story of digital expansion is the story of
creating people into becoming users. The success of digital connectivity
lies in transforming the subjects into users. They are defined by their
presence in these digital systems which access them through
informationally overloaded circuits. In this narrative, digital networks
such as the internet find their cybernetic nodes in digital users, and
the development is thus co-constitutive and interactive, where
technologies course through bodies, converting them into digital nodes,
which, when assembled, form the layered and stacked network of
information transactions.

The problem may well be that the idea of users as agential, and thus in
control of the systems of digital networking in which they are
installed, may well be misleading. Overwritten in informationality and
superseded by the processing power of rapid computing, the user – often
the end point of these technologies – is also *excluded* from
decision-making practices and processes of digital developments. Such a
realization is especially significant given the rhetoric of unlimited
choice, individual customization, filter bubble existence, and network
neighborhood architecture often presented in the guise of human-centric
design of the digital ecosystem. In the following section, I narrate a
symptomatic development of digital technologies in India to show how the
human user, an infrastructural presence in the growth of these
technologies, also remained excluded from their ownership and
decision-making.

The first section has been an exposition on the making of digital access
systems, and the way they *create* and *validate* specific human users
as legitimate operators of these spaces. In the next section, I focus on
the making of this user – how different computational logics, network
principles, and technosocial standards naturalize the making of this
user – and particularly consider the questions of power, agency, and
choice as they unfold in these informationally overloaded systems.

## SECTION II 

## 

## THE PORTRAIT OF YOU AS A USER

In the summer of 2020, Netflix released a globally trending documentary
called *The Social Dilemma* which centered the voices of some of the
people who worked in the early years of the Silicon Valley expansion to
create the monopoly of the GAFA (Google, Amazon, Facebook, Apple)
companies over our attention and data economies. Largely male, white,
and rich, these woke voices presented themselves in a *mea culpa* – they
screwed up, they set out to save the world and ended up ruining it; they
believed in the emancipatory powers of digital technologies but don’t
quite understand how these technologies produced the conditions of dread
that we now see in our digital futures. Apart from the fact that the
documentary failed to address or even acknowledge the voices of
critiques, activists, and researchers who have been actively
foreshadowing the future, it also failed to acknowledge that these
technological problems are not merely technological problems – they are
profoundly human and fiercely intertwined in the shaping of our
collective and singular identities.

As the film went on, the very people now confessing to their oops
moments began positioning themselves as the new warriors who were going
to learn from mistakes (that they consciously made during the times of
emerging tech) and were now going to build better and more humane
technologies to set the world right into order. Each well-curated and
crafted message ended with hope that we will only need new technological
frameworks, new tools of regulation and control of these technologies,
and all would be well. Without any sense of irony, when they talked
about their own personal interventions in the field of data
surveillance, predictive algorithms, and acute information profiling,
they kept resorting to ways of stepping out of information overload.
They did not mention information overload *per se*, but in their
discussions on design hacks, UIX manipulations, data seductions,
algorithmic correlations and profiling, their message was very simple:
these technologies have now produced such an overwhelming amount of data
and information that, at a very human level, it will be impossible to
actually understand, process, and fathom our informational societies
leave alone rebuild them. The message was clear: those who can afford
it, will find data bubbles where they can escape the onslaught of
information overload. The rest of us will just have to bear with it and
put our trust in other technologies that will counter the attacks of the
current technologies.

Alternatively, if we were to extrapolate from their implicit messages,
the only way to fight these current technologies of information overload
is to develop yet more technologies of counter-information overload. In many ways, this debate is
reminiscent of the first propositions by Alan Turing during the Second
World War to break the code of encrypted Nazi information by developing
decryption machines. Alan Turing’s so called ‘Turing Test’ is often
misrepresented as some kind of purity standard that digital computation
has to pass in order to qualify as intelligent. However, a closer
reading of Turing’s essay, ‘Imitation Game’, presents a more interesting
formulation. Turing wasn’t interested so much in whether a computer
network takes the semblance of intelligence or not. He was more invested
in the idea that a computer network can perform a manipulation and
ordering of abstract symbols (like numbers or language) and thus compute
all possible predictions in any value (like a sum or a sentence) and
eventually approximate it in such a way that the process of composing
that value would become intelligible to a human reader.

Turing’s imitation game was not about a computer imitating a human and
fooling us into accepting it as sapient intelligence. He was rather
proposing that computer networks are more adept at *manipulating
structures and symbols that we may think of as human language* and can
thus produce complex results that can no longer be understood or decoded
by human faculties. The imitation machine, then, would be the answer to
the encryption produced by one machine. At the
level of both scale and complexity as well as of speed, the measure of
one machinic process would be another machinic process, or rather, the
best imitation of a machine would be another machine so that when
machines start speaking to each other, the human in the information
circuit would either become unbearably overloaded or outright excluded
from this information transfer.

Turing was effectively warning us that if we
continued to reduce language to nothing more than granular arrangement
of data, strip it off all the different contexts, bodies, and affects
that give it meaning, we would soon find language becoming a paralyzing
rather than a generative force because then the logical machines would
take over. The original proposition of the Turing Test – that in a
purely text-based environment mediated by a digital screen we might no
longer be able to discern whether the person we are talking to is male
or female – is signaling that when it comes to information overload,
*context collapse is the natural mode of informational existence*, that
the individual capacity for discernment, interpretation, and engagement
with information is simultaneously both redundant and futile. So much so
that the very basic identification of the sex of the person, manifest in
so many different ways in embodied interactions, might collapse, thus
introducing in us a fundamental existential crisis where we would no
longer be able to rely on our capacity to tell a truth, to recognize or
engage with truth. The Turing Test was not about being fooled by
machines, but about realizing that we were rapidly emerging into a
system where our human abilities and histories of truth-telling might be
replaced by systems and networks of verification, where the human
subject is both ontologically unstable and epistemologically rewritten.

I invoke Turing to come back to our conversation around information
overload because, as we have seen earlier, nearly all conversations
around information overload have been staged and rehearsed as
technological conditions. They typically invite two pre-wired responses:
one, that if we only created enough regulations and controls, the
technologies would become better; or, two, that we needed to educate and
rehabilitate users into coping and negotiating with information
overload. In both these responses, the user remained either a problem to
be solved or a unit that has to be aligned to the state of continued
crisis that we experience in our unceasing and unrelenting state of
being informational. The user thus gets defined only as a transactional
entity of the computation network. In order to put the questions of
agency, affect, embodiment, and intentions into the conversation around
information overload and usership, I propose the term ‘Youser’ – a
portmanteau of ‘you’ and ‘user’ – in order to anchor the questions of
information overload and the movements that naturalize it as personal
questions that affect the very making of our bodies, selves, identities,
and desires.

The idea of a youser suggests that we need to let go of both the
romantic fantasies of ‘stepping out’ of these networks of information
overload as well as futuristic impulses of finding a ‘cure’, as if this
is just a phase or a temporary state of being. Instead, I hope to map
out specific transitions that digital computation-driven information
overload enables, and suggest an in-betweenness there that might find
resistance, critique, and collectivity, to question informationally
overloaded futures otherwise presented to us as both definite and
inevitable.

## 

## Making of a Youser

The idea of the youser was perhaps most
visible in two almost science-fiction-like predictions that were made in
2014 by two of the biggest digital technology actors, showing us the
imagined future of data-driven realities. Google announced the backing
of a life-extension company called Calico that aimed to cure not disease
but death itself, thus promising immortality. Such immortality would be
achieved through extensive data mining of the human code.
A company that combines biotechnology,
artificial intelligence, deep-learning algorithms, and pharmaceutical
experiments was now sure that deep data mining that allowed for
unprecedented and unfathomable intimacy of code with human biology would
also allow for the production of super drugs to reverse aging and thus
defy death itself.

It was surprising and revealing to large swathes of people who identify
Google largely as a digital information organization company that the
quest of organizing the world’s information is not merely a question of
sorting and processing information: it also entailed converting the
world into information. As Google built larger and more comprehensive
databases where user data got organized, stored, and processed, it also
started creating a profile of the user that both mimicked and overrode
the knowledge that users had about themselves. In the ambition to cure
death, Google’s vast data empire established itself as a digital force
that could rescue the human from its fragile, mortal conditions. In
offering this hope of being saved, Google could immediately trigger a
future that it defined, because it became clear that the older human had
to now step out of the model of being either human or relevant and enter
this new world where Google would be the savior.

That same year, billionaires Peter Thiel and Elon Musk announced their
quest for immortality through singularity by investing in a Thiel
Fellowship that encouraged young innovators to ‘hack’ the human body,
encode it exhaustively into data, and thus find ways of transferring the
data into android smart machines that continued to live on our behalf
long after our bodies were claimed by death.

In both visions, concerned in the end with human mortality, the
human-technology intersections are clear. In Calico’s vision, human
biology is a code that needs to be broken and intercepted so that it can
be infiltrated by genetic information that makes us live beyond our
natural capacities. In Thiel’s proposition, the body is merely a shell
that houses the human: it can be discarded once the ‘essential makeup’
of the human gets transferred to more durable hosts. In this singularity
vision, the human body is clearly being positioned as both undesirable
and accidental and in need of a reengineering that can cure its mortal
uncertainties.

While both seem to mimic science fiction fantasies – and indeed both of
them heavily reference immortality quests drawn from popular and
historical sci-fi – what they also do is signal a changing model of the
human within new data realities. This model of the
human-in-technological-transition, liberating and frightening as it is,
betrays three intertwined tropes in the discourse around technology and
the human, especially with the rise of the digital: the *human as
discrete* from the technological conditions; the *human eroded* through
the technological engagement; and the *human and the technological as
contingent* *and paradoxical in their codependence*.

The ‘discrete human’ has been scathingly critiqued by Asha Achuthan, who
argues as a feminist epistemologist of science and technology that such
a discrete human has been at the heart of the radical transformation
caused by Information and Communication Technologies for Development
(ICT4D) in developing networked countries like India.[^05NIshantShah_84] Achuthan
posits, through a political history of nation-building, that despite the
change in political ideologies of statecraft and governance, even as
technologies of governmentality leapfrogged into different forms, the
human as a ‘user’ always remained discretely removed and separated from
the technological. Achuthan argues that the production of the digital
and human as ontologically discrete, and in need of reconciliation,
perpetuates the idea that technology is prosthetic and dependent upon
human desire and agency for its meaning.

It is a trope that Bruno Latour argues against when he recommends that
we ‘inquire into the existence of things and into all the relations in
which things enter as well as the behaviours and values they exhibit, in
order to exist’.[^05NIshantShah_85] Latour resists imagining technology as neutral,
but also warns us against seeing human existence as the teleological
means of technological forms. He shows that while reified practices of
technology are marked by intention and desire, the singular focus on
transactions and actions as the end point of existence willfully remains
innocent of the politics that accompany the practices that make the
human and the thing coexist as a parity.

The second trope of the human being eroded through technological
engagement is underlined by the contemporary vision of a postapocalyptic
future where the human has been dislocated from its centrality in our
contemporary Anthropocene.[^05NIshantShah_86] It is a vision where the
human-technology interaction is seen as a process of extraction, where
the idea of ‘use’ is not as simple as a human using technology. As Rosi
Braidotti emphasizes in her formulation of the ‘post-human’, the
dislocation of the human from its centrality is not just a conceptual
tool, it is also a recognition that the idea of the human at the center
of the universe, creating use value for all of its resources, is no
longer valid; the human is perhaps more *used* than *using* in the
future.[^05NIshantShah_87]

Malavika Jayaram points out in her analysis of e-governance structures
in emerging networked societies that there is a conceptualization of the
human as a data pod, harvested for data that seeks to *replace* rather
than *represent* the human subject.[^05NIshantShah_88] This is what I have elsewhere
described as *subject of/to technology*,[^05NIshantShah_89] where the very idea of the
human subject has been under renegotiation. On the one hand, the human
is sought to be reconfigured in the digital matrix, and on the other
hand, the human is parsed, processed, and presented only through
interfaces that render it recognizable. The call for rehumanizing
technology, or mapping the human subject, presumes that the human and
the technological are unknown to each other, creating, in the process,
opaque structures of dependence and manipulation.

The third trope sees the human and the technological emerge as
paradoxical, each as the context for the other. As Arjun Appadurai
argues in the unravelling of the conflicting ‘scapes’ that mark
modernity, the fixity of geography and genesis are no longer granted to
the modern migratory subject who encounters technological transactions
in essentially transitory states.[^05NIshantShah_90] The modern subject, for
Appadurai, remains bound in technology and media, which then produces
all-encompassing and inescapable situations that define the ethnic,
ideological, and financial determinants of everyday life.[^05NIshantShah_91]
Appadurai’s argument for thinking of the contemporary moment as a moment
of fusion – where the inextricable nature of our technologized
subjectivity brings forth what Woodhouse and Patton call the
‘technosocial regime’[^05NIshantShah_92] – is important to look at new intersections
of politics, policy, and practice. This technosocial subject resonates
with Donna Haraway’s ironies that define the cyborg. As she says in
*Simians, Cyborgs, and Women*: ‘Irony is about contradictions that do
not resolve into larger wholes, even dialectically, about the tension of
holding incompatible things together because both or all are necessary
and true. Irony is about humour and serious play. It is also a
rhetorical strategy and a political method.’[^05NIshantShah_93] This technosocial
subject, then, does not have the burden of either reconciling or
choosing between staged paradoxes, but finds its existence in a
continuous negotiation between the two.

This technosocial subject is what I call a youser. The youser, defined
by the information overload and circumscribed in the technologies that
support it, is no longer outside the computational networks and no
longer seeks to escape its matrix. However, unlike the dramatic
declarations of Google and Elon Musk, the youser is not constructed
overnight. The production of the youser does not carry the same speed
and scale of many of our popular social media practices where entire
lifetimes get lived and transformed in short-lived trends. Instead, the
production of the youser is an insidious process. It takes a long time,
and when the shifts happen, they happen almost imperceptibly, as
transitions get naturalized and new modes of being and becoming are
established as the status quo.

It is important to call out these transitions in order to unpack the
processes by which the youser is engineered, produced, verified, and
created, and see the possibilities of escape and resistance to this
insidiousness of information overload that we have now come to
experience as the default of our digital lives.

### 

## From Representation to Simulation

In her much cited and often critiqued ‘The Cyborg Manifesto’, Donna
Haraway cryptically introduces the shift from representation to
simulation as marking the transition from ‘comfortable old hierarchical
domination to the scary new networks… (of) informatics of
domination’.[^05NIshantShah_94] Haraway does not expand upon this shift, though she
was already writing with others who were examining simulation as a new
order of ‘reality making’ in a world of symbols that ruled the
abstractions provided by the new social relations tied to science and
technology at the turn of the century. The works of Baudrillard[^05NIshantShah_95] and
Deleuze[^05NIshantShah_96] are often invoked in thinking through the question of
simulation, and the practices of hacking simulations[^05NIshantShah_97] are offered as
a way of breaking the simulations open. I am not taking either of those
routes, nor am I seeking to develop toolkits on how to work with them. I
am more interested in the shift itself – from representation to
simulation – as a computational network phenomenon that controls and
shapes the politics of informationality. While Haraway does not herself
unpack this shift, it is interesting to go down this particular rabbit
hole and understand what simulation might mean within a computational
network.

Namita Malhotra reminds us, in her analysis of born-digital pornographic
objects, that these are essentially simulations.[^05NIshantShah_98] At the back end of
a digital video are numbers. These numbers become pornographic images,
like bodies without organs, only when they are rendered through a
simulation software, such as a web browser or a video player. In fact,
one of the continuous struggles in preparing standards for a unified
language of the web has been that different simulation software
compilations render this material differently, thus reminding us that
there is nothing authentically real about digital objects: they are
inherently ‘fake’ and capable of being ‘faked’.

Duncan Watts complements ‘fake’ digital media in his description of the
mathematical networks that power computation.[^05NIshantShah_99] Watts admits that the
mathematical model of a social network is ‘in place of the real thing’,
and it does not have fidelity to the external world. Computational
networks might have a correlation with the world that they seek to
describe and model, but they are not in a state of mimesis. They are not
‘realities’ that can be understood by measuring them up against a
physical phenomenon or event. The network is an abstraction of modified
information and of the connections made between them. This
nonrepresentational characteristic of the network enables a
decontextualization of the information that it circulates. Thus, even a
stable information set in a network is subject to change as new
connections are made, even though there might be no change in the ‘real
state’ of the individual or phenomenon that is the source of the
information being abstracted. This break from fidelity enables new
conditions within which we measure the veracity of information.

This means that the computational network is necessarily *in a state of
simulation* and can no longer live up to the expectations of the ‘real’
and the ‘truthful’: concepts premised on a representational paradigm
that presumes a material external reality to the represented object, an
external gaze that can be used as a verificatory tool when there is
doubt about the meaning of the representation. To put it bluntly, within
any computation network of simulations everything is ‘fake news’.

Such an eschewing of ontologies of the real is a characteristic of what
Albert-László Barabási calls ‘scale-free networks’.[^05NIshantShah_100] As Barabási
explains, such a network doesn’t only have a lack of fidelity with the
external world, it doesn’t even have any central median value or scale
by which other elements of the network might have an ascertained value.
A scale-free network is thus an indeterminate system, where each element
has a relational value only in the transactional connection that is
established between the two.

*The shift from representation to simulation, then, is not just
aesthetic but ontological.* It invalidates the meaning-making
foundations of our representation-driven expressions and languages. It
propels us straight into the world of ‘deep fakes’ and ‘alternative
facts’, not as aberrations to a human truth, but as new ‘cyborg unities’
that are corrupt, instable, and without fixity. This shift is also not a
futuristic one: it is coded into the very nature of computational
networks that drive our digital communication practices. Thus, the ‘you’
in ‘youser’ of information is created *as* a simulation, a fake, an
approximation without an original. The youser has to be understood as
something created through manipulated information, decontextualized
data, faked circulation, and doctored rendering, needing neither
certainty nor external verification. This youser now needs to be
understood as a ‘cyborg body’ that circulates to create informational
instability. As Haraway had prophesized, ‘we can no longer go back
materially or ideologically’.[^05NIshantShah_101]

## From Authority to Authorization\

Perhaps this shift from representation to simulation is best understood
as a shift from author*ity* to author*ization*, illustrated in the
stories of ‘fake news’ that have been punctuating the landscape in India
regularly.

Take, for example, the extremely charged story of the young political
activist Kanhaiya Kumar. In February 2016, Kumar along with other former
members of the Democratic Students Union organized a protest at the
Jawaharlal Nehru University campus to oppose the capital punishment
handed to Afzal Guru who was convicted of terrorist charges for an
attack on the Indian Parliament in 2001.[^05NIshantShah_102] The protest ended in
clashes with the Hindu nationalist student organization Akhil Bharatiya
Vidyarthi Parishad (ABVP), and videos of the clashes and the events went
viral. Among these witness videos, there was a set of clips circulated
by the Indian news channel Zee News that purportedly showed Kumar and
his fellow protestors shouting ‘anti-India’ slogans. These clips became
the basis for Kumar to be arrested on charges of sedition.

It required forensic digital investigations for the legal courts to
acquit Kumar, as the reports showed that the ‘footage had been tampered
with’.[^05NIshantShah_103] Further scrutiny showed that the audio that was inserted
into the clips was from a different group of people assembled there, and
that it had been overlaid on the video clips of Kumar and his allies.
However, their protest performance, which was asking for the death of
social evils like caste and communalism, got easily doctored into
evidence of seditious sentiments.[^05NIshantShah_104]

Despite the testimonies of Kumar and many neutral witnesses that the
video was not ‘true’, it was nevertheless taken as the authoritative
version of events. It was eventually disproven only when other forms of
contrary digital information came to light. The authority of
truth-telling no longer resided in the human witnesses, but in the
capacity of one technology to *authorize* the other as invalid.

I mark this as a shift from authority to authorization. Authority here
refers to a regime of ‘human-determined’ and negotiated representational
veracity, whereas authorization is a ‘post-human’ techno-cultural force
that combines affect and the logics of computational networks. Neda
Atanasoski and Kalindi Vora in *Surrogate Humanity* provide a critical
framework to understand how the fantasies of assistive technologies –
robots, artificial intelligence – constitute this shift toward what I
shall now call *technologies of authorization*, standing in as
‘surrogates for human workers within a labor system entrenched in racial
capitalism and patriarchy’.[^05NIshantShah_105] Authority specifically looks at the
assemblage where the technology was an instrument assisting human
decision-making; authorization is, on the other hand, a condition where
the human becomes a cog in the larger system of decision-making, and
human discretion becomes one of the many variables that leads to the
final output. Kate Crawford and Vladan Joler, in their visual essay on
‘Anatomy of an AI System’, recognize this shift to authorization as a
‘new form of extractivism’. They write: ‘Many of the assumptions about
human life made by machine learning systems are narrow, normative, and
laden with error. Yet they are inscribing and building those assumptions
into a new world, and will increasingly play a role in how
opportunities, wealth, and knowledge are distributed.’ [^05NIshantShah_106]

This shift from authority to authorization – where we essentially depend
on one set of digital protocols to validate the others – is a telling
symptom of information blackouts. It establishes a digital regime where
the representational touchstones of verification are no longer going to
be enough to determine the veracity of a digital object. And it is worth
noting that, despite the high-profile media trial around this doctoring
of evidence and the disproving of the sentiments, the viral reach and
accelerated spread of the video was so vast that it started a nationwide
rhetoric of branding JNU students as ‘anti-national’ and facilitated
draconian control and clamping down of free speech and organization on
the campus by the right-wing political party in power.

A similar pattern emerges in WhatsApp lynch mobs, where it is not just
the instability of truth in a born-digital simulated object, but the
speed of its circulation that leads to a fabricated reality. So it was
in the case of the well-documented[^05NIshantShah_107] instance in the district of
Bidar, when Mohammad Azam, a software engineer working with Google, had
gone off on a long pleasure ride in his car with a relative and friends.
One of the members of the group, who was visiting from Qatar, had a box
of chocolates with him. At a stop where they were stretching their legs,
when he saw a group of schoolchildren passing by and looking at them, he
offered to share some chocolates.

What the group did not know was that the entire region was bristling
with WhatsApp videos with no provenance warning people of child
abductors. Even though there had been no cases in the villages of
kidnapping, communities were firmly convinced that multiple children had
gone missing. When somebody reported the men stopping in the village and
offering chocolates to children, a mob quickly assembled. They started
abusing the group. The group fled in their car. However, the information
of their arrival had passed much faster than their travel, and in the
next village of Murki, they were faced with an angry mob that forced the
car to stop. The mob dragged the occupants out of the car and beat them
up with sticks and stones. By the time the police arrived, Azam was
dead, while the others had sustained critical injuries and were taken to
the hospital.[^05NIshantShah_108]

Even though the group that was lynched bore all the markings of
affluence and middle-class respectability, the mob insisted on
misreading them as kidnappers. In their police reports, the survivors
from the group talked about how all their explanations, their
identification, and efforts to have a conversation were overridden and
they were framed only as confirmed kidnappers who had come to abduct
children. In other testimonies, people also note how mob participants
saw videos that they had access to and used them as verifying evidence,
claiming that the people in their videos resembled the group that they
were attacking. And even when the group could escape the first mob, the
speed with which information was passed to the connections in the next
village eventually turned fatal for the traveling company.

This predominance of authority of digital objects – without recognizing
them as simulated and potentially modified – and using them as a way of
establishing representational truths, overriding the authority of the
individuals but trusting the authorization of the digital information –
is the way by which the youser become operational. It is no longer a
question of verifying information but of producing information that
pretends to be representing a reality and only ends up simulating the
small world that works through connections, circulations,
recontextualization, and computational realities.

## From Possibility to Probability\

The reason why networked authorization works over human authority and
discretion is not only to do with the life cycle of rumors and speed of
information. It has also to do with the epistemic choices of networked
computation. At the heart of counting are two knowledge-making systems.
One deriving its laws from logic, and the other from mathematics. In
contemporary computation, drawing from Watts’ ‘small world’ hypothesis,
it is clear that we have established logic as the default mode of
meaning-making. With self-containing systems at its core, the logical
world of computation creates a system of referential meaning-making that
depends on deduction and reduces the probability of event occurrence.
Logical probability insists that if a thing happens once, then the
chances of it happening again are large; and if it happens enough times,
it is the naturalized order of things so that other alternatives of
things happening are reduced.

In her work expanding the second order of logic, Maria Manzano argues
that within the first and second order of logic, ‘we will never find a
strongly complete deductive calculus… (because) compactness, which could
be proven from strong completeness, fails’.[^05NIshantShah_109] She draws from the
history of logic to remind us that ‘a complete calculus can never be
obtained’, and yet computational networks continue to demand and affirm
stability and validity of a probability driven approach. Clemens Apprich
characterizes this as the ‘hermeneutical paradox’ where, ‘\[i\]n order
to filter a message out noise, to literally discriminate data to extract
information, the discriminatory patterns within the communication
process have to work behind the scene’.[^05NIshantShah_110] The logical fallacy that
*first* computes all the possibilities and reduces them to a probability
coefficient, and *then* uses that probability coefficient as a way of
predicting and shaping behavior and events, is deeply located in the
four principles of social media as characterized by Jose van Dijck and
Thomas Poell: *programmability*, *popularity*, *connectivity*, and
*datafication*. As they argue, in our data-driven computational
networks, computation now ‘refers to the ability of networked platforms
to render into data many aspects of the world that have never been
quantified before’.[^05NIshantShah_111] These
quantifications, unprecedented in their volume and velocity, being
constantly consolidated and circulated, provide the platform basis for a
whole new structure of *governance* which tries simultaneously to
predict and shape the behavior of the users and redefine our
understanding of both engagement and participation.

With this logic of pattern recognition and probability indices, the
youser becomes a symbolic construction that only makes sense within the
confined and closed universe of a computational network system. The
youser has no fidelity, no engagement, and no faithfulness to the
person(s) that they might refer to. The idea of the youser is not so
much to *describe* the user as it is to *hide* that fact that our
digital networks are more expansive and more opaque than we understand
when we bring them down to mere transactions made by the user.

The creation of the youser is not wedded to these principles of logic.
Yousers are grounded in the mathematical realms of possibility mapping.
The first order of mathematics insists that, at any given space-time
confluence, there is an almost infinite possibility of things that can
happen. Even in making something as simple as a natural number, there is
recognition that the value can be infinitesimally granulated so that it
only approximates the number but never quite achieves it. Or, to make it
more legible, the value of 1 is an infinite possibility of the number
‘0.999999…’ stretched to such infinite levels that it might as well be
1. Mathematical possibility makes transparent the pragmatism of choice,
but in doing so it also denies the precluding of all the other things
that might happen outside of computed events.

This mathematical uncertainty is closer to human perception and
cognition than it is to the logical certitude of computation networks.
This uncertainty replaces *scale* with *intensity* and *speed* with
*depth*, offering a different register of information within the digital
realms – imagining a network of quantum computers invested in
calculating different possibilities of human negotiation rather than
predicting the scripted routes of controlled behavior. It privileges the
making and structuring of authority, and the negotiations with it,
rather than the deployment and penalization of authorization that is
central to shutdowns.

Perhaps this tension between logic and mathematics is the most visible
in conspiracy theories that accompany particular crises. Take the
terrifying Facebook streaming of two consecutive mass shootings that
occurred at mosques in Christchurch, New Zealand, during Friday prayer
on March 15, 2019. The attacker, a white supremacist, was charged and
convicted of 51 murders, 40 attempted murders, and terrorist
activity.[^05NIshantShah_112] In an attempt to reconcile the population with this
heinous act, both the video and the manifesto that it was linked to were
banned in New Zealand,[^05NIshantShah_113] effectively putting a temporary lockdown
both on the video’s circulation and its capacity to make connections.

However, the erasure of the perpetrator as a Facebook user via modes of
internet censorship did not stop a global response and viral consumption
of the phenomenon.[^05NIshantShah_114] Additionally, it led to an enormous conspiracy
theory group emerging in far-right global mainstream media as well as on
the fertile grounds of Reddit where detailed forensic analysis of the
video insisted that the entire shooting and its subsequent regulation
were staged in order to forward a ‘liberal left agenda’ that would take
guns away from people.[^05NIshantShah_115] This wasn’t just a small faction, but these
conspiracy theories found huge traction in different media forms and in
viral posters and memes that offered ‘rational’ evidence to their
irrational analyses.

This particular form of resurgence of a user who has been removed by
regulation as a youser that is still circulating and replicating is in
the realm of the mathematical possibility that abounds the digital web,
so that even when faced with factual or logical resistance, the
informationality of the youser only grows in intensity and popularity.
From anti-vaccine movements to flat-earth conventions and global climate
change deniers, these dabblers in possibility mapping continue to
exploit the strength of correlative causality embedded in digital
networks to create these alternative ‘small worlds’. They find each
other through weak ties of topical interest and the algorithmic
correlation of weak ties to form a sustained community consolidated on
unregulated platforms, where they emerge as a collective force rather
than merely fringe elements of irrational thought. Thus the ‘content possibility’ that enables irrational
argumentation, conspiracy theories, and absurd fantasies, none of which
get either detected or filtered by the computational networks of weak
ties because such networks are only tied to ‘network probability’ that
maintains a mythical neutrality toward the information, the theories,
and the fantasies being circulated.

The networks pretending agnosticism to content, its traffic and the
management of its connections allow for such discourses to emerge
uncontrolled into a domain of speculation. Networks facilitate these
practices even as they try to establish regulation over social media
platforms. And even when legal and governance measures, depending on the
fallacy of logical erasure, remove users from these platforms, the
networks of circulation and possibility continue to proliferate and
replicate such yousers on the social web’s horizons of possibility.

## 

## From Memory to Storage

## 

Computational networks encourage overproduction of information. This
happens in part by making users encounter their own information and its
relationships, and partly by circulating all information at the same
scale and speed, making it almost impossible to take parsing or
verifying practices as human practices. In fact, the new mode of
countering or blocking information is to flood the topics, hashtags, or
viral events with counter-information and misinformation so that human
users are so overwhelmed by the information that they disengage from it.
This disengagement, however, does not mean a lack of information. It
merely means that the responsibility of presenting this information is
now given to machine learning and customization practices that we call
artificial intelligence. It establishes a new model of information
circulation, where the human user is created by an algorithmic data
harvest and the targeted information provided to the user is also
curated and filtered by another algorithmic network. The human in this
computational network – sandwiched between two sets of self-verifying
and mutually complementary algorithms – has diminished the agencies that
protect free speech and expression.

This is the death of memory – the capacity to remember, the ability to
forget, and the need for willfully remembering wrong. This is the
relegation of the youser into a database, an information stream that is
beyond the capacity of the human, trusted only to the verification and
authorization principles of algorithmic machines that mimic human agency
and make machine decisions. The role of the digital self is to be bereft
of memory, but in the midst of abundance of storage. The responsibility
and capacity for memory has firmly been relegated to other things that
perform that task.

Both in our bodily practices and data preservation, we persist in being
creatures of storage. A look at our hard drives will tell us that we
have stored more data than we remember or will ever read. A glance at
our digital histories and archives shocks us because just the storage of
our self has taken up so much data property that increasingly we are
unable to read anything more than the data that we have produced – we
have become queries that retrieve the data that algorithms sort for us.
Our relationship with our data, as informationally overloaded subjects,
is necessarily one of *dis*information. Given the volume, velocity, and
vectors of our data, it is now a given that everything we can know about
our data is wrong, and that we will be corrected only by the algorithms
of storage that will do our remembering for us. We complain about
information overload now, not because we cannot remember enough, but
because we are aware that digital storage is always going to exceed our
capacities of memory, and hence we see ourselves moving into storage.
What you can tell about yourself is now wrong. And if your memory is
challenged with storage, you know that you are going to lose the battle.

## INSIDIOUSNESS OVERLOAD

In contemporary discourse around the future of digital technologies and
the lives lived in those futures, many anxieties get foregrounded.
Questions of automation, machine ethics, information extraction, data
surveillance, algorithmic censorship, infrastructural silencing, and
violence by misinformation have not only become theoretical speculations
but lived realities in our increasingly networked societies. The
formulation and intervention in these areas are rich with different
multilayered approaches and poly-vocal narratives that give attention to
the social, technological, embodied, affective, and material
consequences of these unfoldings.

Most of these formulations and interventions are circumscribed by a few
characteristics. They recognize the problems post facto. The focus on
case studies, production of actual events, and reliance on a case-driven
narrative invariably means that the problem with all its violence and
consequences, often borne by those who are the most vulnerable, is
allowed to flourish. The scale with which these problems proliferate in
digital circuits also means that countermeasures are slow, lag behind,
and not effective in reaching the root cause of these problems.

The identification of the frictions and dangers focuses on dramatic
unfolding. While many of these problems are quite spectacular (as also
referenced in this essay), it is obvious that these spectacles are
neither representative nor comprehensive in understanding the
underpinning principles and motivations that create them. They are both,
invisible and offered as natural progression, thus making it difficult
to identify them when they are being operationalized. In reality,
despite knowing these concerns, we become willing actors contributing to
the development of these shifts because of the incentives and
gratifications that are offered.

There is a tacit understanding that while these technological
developments are problematic, they are aimed at the social good and need
to be protected and championed. From the market logic of *too big to
fail* to the technosocial framing of *technology for social good* or its
predecessor concept of *information technologies for development*, most
definitions of both good development and bigness are unable to recognize
their structurally exclusionary and discriminatory character. The
conventional counter to technological problems is inevitably more
technology of a similar kind with a slightly different orientation.

What remains problematic to these circuits is the location of the human
within them. Visual and popular narratives insist on ease of access and
convenience as the end point of users’ interactions with these
technologies, and continue under the seduction of the visual interface
to create new operational conditions that nudge, train, and produce
users into becoming a different kind of subject.

The lens of information overload that I have presented here offers two
ways of breaking through these much-repeated modes of inquiry and
intervention. In information overload, I identify the axiom that
information overload is a computational network necessity, not a human
one. In showing the different ways by which information overload is
being naturalized, I show how it creates a subject that is rendered
vulnerable, fragile, helpless, and in need of rescuing by the very
technologies that create this subject position to begin with.

In the shift from user to youser, I show how the technological user is
not just being led into new forms of practice and transactions, but
rather into an entirely new mode of being and becoming. By making the
principles that engineer these transitions explicit, I show that the
youser is already in the making, and that access to and of digital
technologies has to be complicated to produce a more nuanced idea of who
is being accessed, for what purpose, and how this access is creating a
new template of a technological user.

Through all this, I want to emphasize that the digital condition (as
opposed to digital technologies) is insidious, and it doesn’t merely
amplify or augment reality, but replaces it, gently, without fanfare,
and then demands and forces us to accept it, spending our energy and
focus in filling up the positions and profiles that are created in this
new reality. This essay is an attempt to provide a framework where we
can think through our digital conditions to create spaces for
intervention rather than just making meaning, and to make transparent
the ways by which we are seduced into becoming yousers of an insidious
informationality.

The history of the digital conditions has been written as the history of
digital access. However, digital access is an insidious process that
diverts our focus and energy on explicating, understanding, and making
meaning of it. In this role, we are always going to be informationally
overloaded, temporally lagging, and structurally reactive, falling into
predicted futures with scripts where our roles have already been
written.

[^05NIshantShah_1]: The video, a piece of misogynist vitriol, was later deleted by the
    original uploader. While many clone copies exist, I find it
    important not to center his voice and give him more hyperlink
    attention. However, the following write-up in *The* *Free Press
    Journal* offers a comprehensive summary of the case at hand. See
    ‘Remember Agrima Joshua’s Vile Abuser Shubham Mishra? He’s Out on
    Bail Now’, *The Free Press Journal*, 18 August 2020,

    <a href="https://www.freepressjournal.in/entertainment/remember-agrima-joshuas-vile-abuser-shubham-mishra-hes-out-on-bail-now">https://www.freepressjournal.in/entertainment/remember-agrima-joshuas-vile-abuser-shubham-mishra-hes-out-on-bail-now</a>.

[^05NIshantShah_2]: *The Quint* published a detailed report on the story. See, ‘Comic
    Joshua Gets Rape Threat for Joke on Chhatrapati Shivaji’, *The
    Quint*, 13 July 2020,
    <a href="https://www.thequint.com/news/india/comic-agrima-joshua-gets-rape-threat-for-joke-on-chhatrapati-shivaji">https://www.thequint.com/news/india/comic-agrima-joshua-gets-rape-threat-for-joke-on-chhatrapati-shivaji</a>.

[^05NIshantShah_3]: ‘Agrima Joshua Case: Maha HM Anil Deshmukh Asks Mumbai Police to
    Take Legal Action against Comedian over Chhatrapati Shivaji Maharaj
    Remark’, *The Free Press Journal*, 11 July 2020, []{#_Hlk105225732
    .anchor}https://www.freepressjournal.in/mumbai/agrima-joshua-case-maha-hm-anil-deshmukh-asks-mumbai-police-to-take-legal-action-against-comedian-over-chhatrapati-shivaji-maharaj-remark.

[^05NIshantShah_4]: ‘Agrima Joshua Row: Comedian Posts Video Apologizing to Members of
    Political Parties’, *The Free Press Journal*, 11 July 2020,
    <a href="https://www.freepressjournal.in/india/agrima-joshua-row-comedian-posts-video-apologizing-to-members-of-political-parties">https://www.freepressjournal.in/india/agrima-joshua-row-comedian-posts-video-apologizing-to-members-of-political-parties</a>.

[^05NIshantShah_5]: Belinda Goldsmith and Meka Beresford, ‘India Most Dangerous
    Country for Women with Sexual Violence Rife - Global Poll’,
    *Reuters*, 26 June 2018,
    <a href="https://www.reuters.com/article/women-dangerous-poll-idINKBN1JM076">https://www.reuters.com/article/women-dangerous-poll-idINKBN1JM076</a>.

[^05NIshantShah_6]: ‘2020 World Press Freedom Index: “Entering a Decisive Decade for
    Journalism, Exacerbated by Coronavirus”’, Reporters Without Borders,
    <a href="https://rsf.org/en/2020-world-press-freedom-index-entering-decisive-decade-journalism-exacerbated-coronavirus">https://rsf.org/en/2020-world-press-freedom-index-entering-decisive-decade-journalism-exacerbated-coronavirus</a>.

[^05NIshantShah_7]: Tanvi Akhauri, ‘Shubham Mishra Has Been Arrested, but the Problem
    of Offence-Taking Still Persists’, *shethepeople*, 13 July 2020,
    <a href="https://www.shethepeople.tv/home-top-video/shubham-mishra-arrested-agrima-joshua/">https://www.shethepeople.tv/home-top-video/shubham-mishra-arrested-agrima-joshua/</a>.

[^05NIshantShah_8]: Aaron Blake, ‘Kellyanne Conway Says Donald Trump’s Team Has
    ‘Alternative Facts.’ Which Pretty Much Says it All’, *The Washington
    Post*, 22 January 2017,
    <a href="https://www.washingtonpost.com/news/the-fix/wp/2017/01/22/kellyanne-conway-says-donald-trumps-team-has-alternate-facts-which-pretty-much-says-it-all/">https://www.washingtonpost.com/news/the-fix/wp/2017/01/22/kellyanne-conway-says-donald-trumps-team-has-alternate-facts-which-pretty-much-says-it-all/</a>.

[^05NIshantShah_9]: Eccles. 12:12 Christian Standard Bible.

[^05NIshantShah_10]: Ann M. Blair, *Too Much to Know: Managing Scholarly Information
    before the Modern Age*, New Haven and London*:* Yale University
    Press, 2010; all citations in this paragraph are from pp. 14–16.

[^05NIshantShah_11]: Kathleen Fitzpatrick, *Planned Obsolescence:* *Publishing,
    Technology, and the Future of the Academy*, New York and London: New
    York University Press, 2011.

[^05NIshantShah_12]: Mark Rose, *Authors and Owners: The Invention of Copyright*,
    Cambridge, Massachusetts: Harvard University Press, 1993.

[^05NIshantShah_13]: Virginia Woolf, *A Room of One’s Own*, London: Hogarth Press,
    1929.

[^05NIshantShah_14]: Kate Millett, *Sexual Politics*, Garden City, New York: Doubleday
    & Co., 1970.

[^05NIshantShah_15]: Carolyn Marvin, *When Old Technologies Were New: Thinking About
    Electric Communication in the Late Nineteenth Century*, Oxford:
    Oxford University Press, 1988, p. 81.

[^05NIshantShah_16]: Alvin Tofler, *Future Shock*, New York: Random House, 1970.

[^05NIshantShah_17]: Nike Y2K - Jogger, <a href="https://www.youtube.com/watch?v=q%5C_7YmvH3pYw">https://www.youtube.com/watch?v=q\_7YmvH3pYw</a>.

[^05NIshantShah_18]: Bruno Latour, ‘A Collective of Humans and Nonhumans: Following
    Daedalus’s Labyrinth’, in Bruno Latour, *Pandora’s Hope: Essays on
    the Reality of Science Studies*, Cambridge, Massachusetts: Harvard
    University Press, 1999, pp. 183–184.

[^05NIshantShah_19]: Susan Leigh Star, ‘The Ethnography of Infrastructure’, *American
    Behavioral Scientist* 43.5 (1999): 377–391.

[^05NIshantShah_20]: Wolfgang Ernst, *Digital Memory and the Archive*,* *Minneapolis:
    University of Minnesota Press, 2013, p. 57.

[^05NIshantShah_21]: Wolfgang Ernst, *Sonic Time Machines: Explicit Sound, Sirenic
    Voices, and Implicit Sonicity*,* *

    Amsterdam: University of Amsterdam Press, 2016, pp. 64, 80, xxii.

[^05NIshantShah_22]: Garnet Hertz and Jussi Parikka, ‘Zombie Media: Circuit Bending
    Media Archaeology into an Art Method’, *Leonardo* 45.5 (2012):
    424–430.

[^05NIshantShah_23]: Lauren Berlant, *Cruel Optimism*, Durham: Duke University Press,
    2011, p. 3.

[^05NIshantShah_24]: Wendy Hui Kyong Chun, *Updating to Remain the Same: Habitual New
    Media*, Cambridge, Massachusetts and London: The MIT Press, 2016.

[^05NIshantShah_25]: Chun, *Updating to Remain the Same*, p. 70.

[^05NIshantShah_26]: Chun, *Updating to Remain the Same*, p.74.

[^05NIshantShah_27]: Aarefa Johari, ‘Gujarat Internet Ban: On Day Six, Citizens Have
    Had Enough of Being Patronized by the State’, *Scroll.in*, 1
    September 2015,
    <a href="https://scroll.in/article/752538/gujarat-internet-ban-on-day-six-citizens-have-had-enough-of-being-patronised-by-the-state">https://scroll.in/article/752538/gujarat-internet-ban-on-day-six-citizens-have-had-enough-of-being-patronised-by-the-state</a>.

[^05NIshantShah_28]: Pooja Thomas, ‘Museum as Metaphor: The Politics of an Imagined
    Ahmedabad’, in Arvind Rajagopal and Anupama Rao (eds) *Media and
    Utopia: History, Imagination and Technology*, New York: Routledge,
    2017, pp. 133–148.

[^05NIshantShah_29]: Examining this phenomenon of lockdowns and internet shutdowns to
    show how ‘disinformation’ continues to travel in these shutdowns to
    shape state propaganda and messaging, I have also written about this
    case in another journal paper: Nishant Shah, ‘(Dis)information
    Blackouts: Politics and Practices of Internet Shutdowns’,
    *International Journal of Communication* 15 (2021): 2693–2709.

[^05NIshantShah_30]: To know more about the political context of the Patidar
    community, see Gopal Kateshiya, ‘Gujarat Protests: Who are the
    Patidars, and why are they Angry’, *The Indian Express*, 27 August
    2015,
    <a href="https://indianexpress.com/article/explained/simply-put-who-are-gujarats-patidars-and-why-are-they-angry/">https://indianexpress.com/article/explained/simply-put-who-are-gujarats-patidars-and-why-are-they-angry/</a>.

[^05NIshantShah_31]: Ashish Chauhan, ‘Jat Fire Tempts Patidars to Action’, *The Times
    of India*, 22 February 2016,
    <a href="https://timesofindia.indiatimes.com/city/ahmedabad/jat-fire-tempts-patidars-to-action/articleshow/51086440.cms">https://timesofindia.indiatimes.com/city/ahmedabad/jat-fire-tempts-patidars-to-action/articleshow/51086440.cms</a>.

[^05NIshantShah_32]: ‘Patidar Reservation: Social Media Spreading Sardar Patel
    Movement Like Wild Fire’, *DNA*, 22 August 2015,
    <a href="http://www.dnaindia.com/india/report-patidar-reservation-social-media-spreading-sardar-patel-movement-like-wild-fire-2117056">http://www.dnaindia.com/india/report-patidar-reservation-social-media-spreading-sardar-patel-movement-like-wild-fire-2117056</a>.

[^05NIshantShah_33]: ‘Stir Over OBC Status: Govt Proposes Talks; Hardik Plays
    Hardball’, *The Indian Express*, 23 August 2015,
    <a href="http://indianexpress.com/article/india/india-others/stir-over-obc-status-govt-proposes-talks-hardik-patel-plays-hard-ball/">http://indianexpress.com/article/india/india-others/stir-over-obc-status-govt-proposes-talks-hardik-patel-plays-hard-ball/</a>.

[^05NIshantShah_34]: ‘Gujarat Shuts Down Internet during Exams’, *The Hindu*, 29
    February 2016,
    <a href="http://www.thehindu.com/todays-paper/tp-miscellaneous/tp-others/gujarat-shuts-down-internet-during-exam/article8294672.ece">http://www.thehindu.com/todays-paper/tp-miscellaneous/tp-others/gujarat-shuts-down-internet-during-exam/article8294672.ece</a>.

[^05NIshantShah_35]: ‘The Information Technology Act, 2000’, *The Gazette of India*,
    June 2000,
    <a href="https://www.indiacode.nic.in/bitstream/123456789/13116/1/it%5C_act%5C_2000%5C_updated.pdf">https://www.indiacode.nic.in/bitstream/123456789/13116/1/it\_act\_2000\_updated.pdf</a>.

[^05NIshantShah_36]: See ‘The Information Technology Act, 2000’, p. 4 (unauthorised
    access), p. 6 (storage, retention, and retrieval), p. 9 (licensing
    and public access), p. 11 (availability and perpetuity), p. 15
    (security and legitimation), p. 15 (denial and maleficent blockage
    of access), p. 20 (privacy and replication of information), p. 22
    (markers of public space). The page numbers here correspond to the
    actual gazette and can be found at:
    <a href="https://eprocure.gov.in/cppp/rulesandprocs/kbadqkdlcswfjdelrquehwuxcfmijmuixngudufgbuubgubfugbububjxcgfvsbdihbgfGhdfgFHytyhRtMjk4NzY=%5C#:%5C~:text=[9th%20June,%202000]%20An,communication%20and%20storage%20of%20information,">https://eprocure.gov.in/cppp/rulesandprocs/kbadqkdlcswfjdelrquehwuxcfmijmuixngudufgbuubgubfugbububjxcgfvsbdihbgfGhdfgFHytyhRtMjk4NzY=\#:\~:text=%5B9th%20June%2C%202000%5D%20An,communication%20and%20storage%20of%20information%2C</a>.

[^05NIshantShah_37]: Nishant Shah, ‘In Access: Digital Video and the User’, in Joshua
    Neves and Bhaskar Sarkar (eds) *Asian Video Cultures: In the
    Penumbra of the Global*, Durham: Duke University Press, 2017, pp.
    114–130.

[^05NIshantShah_38]: ‘The Information Technology (Amendment) Act, 2008’, *The Gazette
    of India*, February 2009, p. 3,
    <a href="https://eprocure.gov.in/cppp/rulesandprocs/kbadqkdlcswfjdelrquehwuxcfmijmuixngudufgbuubgubfugbububjxcgfvsbdihbgfGhdfgFHytyhRtMTk4NzY=">https://eprocure.gov.in/cppp/rulesandprocs/kbadqkdlcswfjdelrquehwuxcfmijmuixngudufgbuubgubfugbububjxcgfvsbdihbgfGhdfgFHytyhRtMTk4NzY=</a>.

[^05NIshantShah_39]: ‘The Information Technology Act, 2000’.

[^05NIshantShah_40]: ‘The Information Technology Act, 2000’, p. 12.

[^05NIshantShah_41]: ‘The Information Technology (Amendment) Act, 2008’, p. 27.

[^05NIshantShah_42]: ‘The Information Technology (Amendment) Act, 2008’, pp. 27, 29.

[^05NIshantShah_43]: Douglas C. Engelbart, ‘Augmenting Human Intellect: A Conceptual
    Framework’, Summary Report, SRI Project No. 3578, Stanford Research
    Institute, October 1962,
    <a href="https://www.dougengelbart.org/content/view/138">https://www.dougengelbart.org/content/view/138</a>.

[^05NIshantShah_44]: Adam Fisher documents the process by which Douglas Engelbart
    pulled off the presentation that was retroactively dubbed ‘The
    Mother of All Demos’, and paved the way for Alan Kay’s historical
    presentation. See Adam Fisher, *Valley of Genius: The Uncensored
    History of Silicon Valley (As Told by the Hackers, Founders, and
    Freaks Who Made it Boom)*, New York and Boston: Twelve (The Hatchett
    Group), 2018.

[^05NIshantShah_45]: Douglas C. Engelbart, ‘Games That Teach the Fundamentals of
    Computer Operation’, *IRE Transactions on Electronic Computers*,
    March 1961,
    <a href="https://ia800203.us.archive.org/13/items/GamesThatTeachFundamentalComputerOperationsEngelbart/1961-Games-That-Teach-Fundamental-Computer-Operations-Engelbart.pdf">https://ia800203.us.archive.org/13/items/GamesThatTeachFundamentalComputerOperationsEngelbart/1961-Games-That-Teach-Fundamental-Computer-Operations-Engelbart.pdf</a>.

[^05NIshantShah_46]: Swami Manohar, ‘The Simputer: Access Device for the Masses’, p.
    1, <a href="http://www.simputer.org/simputer/history/paper.pdf">http://www.simputer.org/simputer/history/paper.pdf</a>.

[^05NIshantShah_47]: Manohar, ‘The Simputer’, p. 1.

[^05NIshantShah_48]: Manohar, ‘The Simputer’, p. 2.

[^05NIshantShah_49]: Manohar, ‘The Simputer’, p. 3.

[^05NIshantShah_50]: Namita Malhotra, *Porn: Law, Video & Technology*, Bangalore: The
    Centre for Internet & Society, 2011.

[^05NIshantShah_51]: Shah, ‘In Access: Digital Video and the User’.

[^05NIshantShah_52]: Shah, ‘In Access: Digital Video and the User’.

[^05NIshantShah_53]: *Avnish Bajaj* v *State (NCT of Delhi)*, 105 DRJ 721 (2008). The
    presiding judge was Justice S. Muralidhar. For details, see
    <a href="https://indiankanoon.org/doc/309722/">https://indiankanoon.org/doc/309722/</a>.

[^05NIshantShah_54]: Shah, ‘In Access: Digital Video and the User’.

[^05NIshantShah_55]: Wendy Hui Kyong Chun, ‘The Enduring Ephemeral, or the Future is a
    Memory’, *Critical Inquiry* 35.1 (2008): 148–171.

[^05NIshantShah_56]: Ariana Rodriguez, ‘India Bans Adult Cartoon Site
    SavitaBhabhi.com’, *XBIZ Newswire*, 28 June 2009,
    <a href="http://newswire.xbiz.com/view.php?id=109797">http://newswire.xbiz.com/view.php?id=109797</a>.

[^05NIshantShah_57]: Sruthijith K.K., ‘Govt Bans Popular Toon Porn Site’, *Hindustan
    Times*, 20 June 2009,
    <a href="https://www.hindustantimes.com/entertainment/govt-bans-popular-toon-porn-site/story-M7UO7XgStS9Cfrvfziok6J.html">https://www.hindustantimes.com/entertainment/govt-bans-popular-toon-porn-site/story-M7UO7XgStS9Cfrvfziok6J.html</a>.

[^05NIshantShah_58]: Vinita Chaturvedi, ‘I Keep a Low Profile to Promote Savita Bhabhi
    Better: Puneet Agarwal’, *The Times of India*, 11 April 2013,
    <a href="https://timesofindia.indiatimes.com/entertainment/hindi/bollywood/news/i-keep-a-low-profile-to-promote-savita-bhabhi-better-puneet-agrawal/articleshow/19493624.cms">https://timesofindia.indiatimes.com/entertainment/hindi/bollywood/news/i-keep-a-low-profile-to-promote-savita-bhabhi-better-puneet-agrawal/articleshow/19493624.cms</a>.

[^05NIshantShah_59]: For details, see <a href="https://en.wikipedia.org/wiki/Rohith%5C_Vemula">https://en.wikipedia.org/wiki/Rohith\_Vemula</a>.

[^05NIshantShah_60]: Ashish Rajadhyaksha, *The Last Cultural Mile: An Inquiry into
    Technology and Governance in India*, Bangalore: The Centre for
    Internet & Society/Researchers@Work, 2011.

[^05NIshantShah_61]: Duncan J. Watts, ‘Networks, Dynamics, and the Small‐World
    Phenomenon’, *American Journal of Sociology* 105.2 (1999): 493–527.

[^05NIshantShah_62]: Martin Warnke and Carmen Wedemeyer, ‘Documenting Artistic
    Networks: Anna Oppermann’s Ensembles are Complex Networks!’,
    *Leonardo* 44.3 (2011): 258–259.

[^05NIshantShah_63]: *MySpace Inc.* v *Super Cassettes Industries Ltd.*, 236 DLT 478
    (DB) (23 December 2016). The case was presided over by Justices S.
    Ravindra Bhat, Deepa Sharma. For details, see
    <a href="https://indiankanoon.org/doc/12972852/">https://indiankanoon.org/doc/12972852/</a>.

[^05NIshantShah_64]: Cyril Sam and Paranjoy Guha Thakurta, ‘Part 1: Is Facebook in
    India Truly Independent of Political Influence? Not Really – It has
    Backed Modi and BJP’, NewsClick, 22 November 2018,
    <a href="https://www.newsclick.in/part-1-facebook-india-truly-independent-political-influence">https://www.newsclick.in/part-1-facebook-india-truly-independent-political-influence</a>.

[^05NIshantShah_65]: ‘Notification’, New Delhi, 11 April 2011, *The Gazette of India
    Extraordinary* Part II-Sec.3(i),
    <a href="https://dispur.nic.in/itact/it-intermediaries-guidelines-rules-2011.pdf">https://dispur.nic.in/itact/it-intermediaries-guidelines-rules-2011.pdf</a>.

[^05NIshantShah_66]: ‘Notification’, 3 (1).

[^05NIshantShah_67]: ‘Notification’, 3 (3 b).

[^05NIshantShah_68]: ‘Notification’, 3 (11).

[^05NIshantShah_69]: Rishabh Dara, ‘Intermediary Liability in India: Chilling Effects
    on Free Expression on the Internet 2011’, *The Centre for Internet
    &* Society, 10 April 2012,
    <a href="https://cis-india.org/internet-governance/intermediary-liability-in-india">https://cis-india.org/internet-governance/intermediary-liability-in-india</a>.

[^05NIshantShah_70]: Dara, ‘Intermediary Liability in India’.

[^05NIshantShah_71]: Anna Lauren Hoffmann, Nicholas Proferes and Michael Zimmer,
    ‘“Making the World More Open and Connected”: Mark Zuckerberg and the
    Discursive Construction of Facebook and its Users’, *New Media and
    Society* 20.1(2016): 199–218.

[^05NIshantShah_72]: Lauren Smiley, ‘How India Pierced Facebook’s Free Internet’,
    *Wired*, 1 February 2016,

    <a href="https://www.wired.com/2016/02/how-india-pierced-facebooks-free-internet-program/">https://www.wired.com/2016/02/how-india-pierced-facebooks-free-internet-program/</a>.

[^05NIshantShah_73]: ‘TRAI Tells Reliance to Put Facebook’s Free Basics Platform on
    Hold’, *The Free Library*, 23 December 2015,
    <a href="https://www.thefreelibrary.com/TRAI+tells+Reliance+to+put+Facebook&#39;s+Free+Basics+platform+on+hold-a0438721289">https://www.thefreelibrary.com/TRAI+tells+Reliance+to+put+Facebook%27s+Free+Basics+platform+on+hold-a0438721289</a>.

[^05NIshantShah_74]: <a href="https://savetheinternet.in/">https://savetheinternet.in/</a>.

[^05NIshantShah_75]: Nimisha Jaiswal, ‘Why Indians Are Turning Down Facebook’s Free
    Internet’, *GlobalPost*, 13 January 2016,
    <a href="https://theworld.org/stories/2016-01-13/why-indians-are-turning-down-facebooks-free-internet">https://theworld.org/stories/2016-01-13/why-indians-are-turning-down-facebooks-free-internet</a>.

[^05NIshantShah_76]: ‘Indian Comedians Explain Net Neutrality in Free Basics Battle’,
    *Web We Want*, 13 January 2016,
    <a href="https://webwewant.org/news/indian-comedians-explain-net-neutrality/">https://webwewant.org/news/indian-comedians-explain-net-neutrality/</a>.

[^05NIshantShah_77]: Rahul Bhatia, ‘The Inside Story of Facebook’s Biggest Setback’,
    *The Guardian*, 12 May 2016,
    <a href="https://www.theguardian.com/technology/2016/may/12/facebook-free-basics-india-zuckerberg">https://www.theguardian.com/technology/2016/may/12/facebook-free-basics-india-zuckerberg</a>.

[^05NIshantShah_78]: Mohul Ghosh, ‘Is \#SabKaInternet a Deliberate Attempt by COAI to
    Confuse People?’, *Trak.in*, 2 December 2016,
    <a href="https://trak.in/tags/business/2015/04/22/sabkainternet-deliberate-attempt-coai-to-confuse/">https://trak.in/tags/business/2015/04/22/sabkainternet-deliberate-attempt-coai-to-confuse/</a>.

[^05NIshantShah_79]: Nash David, ‘Digital ‘Equality’ Not So Equal: Is an Aggressive
    Facebook Turning Free Basics into a Movement?’, *Firstpost*, 24
    December 2015,

    <a href="https://www.firstpost.com/india/is-an-aggressive-facebook-turning-free-basics-into-a-movement-2557360.html">https://www.firstpost.com/india/is-an-aggressive-facebook-turning-free-basics-into-a-movement-2557360.html</a>.

[^05NIshantShah_80]: Telecom Regulatory Authority of India, ‘Prohibition of
    Discriminatory Tariffs for Data Services Regulations, 2016’,
    *Gazette of India*, 8 February 2016,
    <a href="https://trai.gov.in/sites/default/files/Regulation%5C_Data%5C_Service.pdf">https://trai.gov.in/sites/default/files/Regulation\_Data\_Service.pdf</a>.

[^05NIshantShah_81]: ‘Prohibition of Discriminatory Tariffs’, p. 10.

[^05NIshantShah_82]: ‘PM’s Remarks at the Launch of Digital India Week’, 1 July 2015,
    <a href="https://www.narendramodi.in/pm-s-remarks-at-the-launch-of-digital-india-week-175128">https://www.narendramodi.in/pm-s-remarks-at-the-launch-of-digital-india-week-175128</a>.

[^05NIshantShah_83]: Internet Freedom Foundation, ‘About IFF’,
    <a href="https://internetfreedom.in/about/">https://internetfreedom.in/about/</a>.

[^05NIshantShah_84]: Asha Achuthan, *Re:Wiring Bodies*, Bangalore: Researchers@Work
    and The Centre for Internet & Society, 2012.

[^05NIshantShah_85]: Bruno Latour, *An* *Inquiry into Modes of Existence: An
    Anthropology into the Moderns*, Cambridge, Massachusetts: Harvard
    University Press, 2013, p. 93.

[^05NIshantShah_86]: Eric Hörl in exchange with Paul Fiegelfeld and Cornelia Kastelan,
    ‘The Anthropocenic Illusion: Sustainability and the Fascination of
    Control’, in Christoph Behnke, Cornelia Kastelan, Valerie Knoll, and
    Ulf Wuggenig (eds) *Art in the Periphery of the Center*, Berlin:
    Sternberg Press, 2015, pp. 352–368.

[^05NIshantShah_87]: Rosi Braidotti, *The Posthuman*, New York; Wiley, 2013.

[^05NIshantShah_88]: Malavika Jayaram, ‘India’s Big Brother Project: The World’s
    Largest Biometrics Identity Program’, *Boston Review*, 19 May 2014,
    <a href="https://bostonreview.net/world/malavika-jayaram-india-unique-identification-biometrics">https://bostonreview.net/world/malavika-jayaram-india-unique-identification-biometrics</a>.

[^05NIshantShah_89]: Nishant Shah, ‘Subject to Technology: Internet Pornography,
    Cyber-terrorism and the Indian State’, *Inter-Asia Cultural Studies*
    8.3 (2007): 349–366.

[^05NIshantShah_90]: Arjun Appadurai, *Modernity at Large: Cultural Dimensions of
    Globalization*, Minneapolis: University of Minnesota Press, 1996.

[^05NIshantShah_91]: Arjun Appadurai’s notion of scapes looks at the boundlessness of
    subjectivity in the age of late modernity. Appadurai posits that the
    two fixities of geography and genesis are no longer granted to the
    modern migratory subject who encounters modernity and its value
    systems in a state of transitory transactions. This produces the
    subject as caught in various binds of ethnicity, ideology, and
    technology that produce it at the center of distinct and divisive
    contrary forces. See Appadurai, *Modernity at Large*.

[^05NIshantShah_92]: Woodhouse and Patton describe technosociality as ‘a system where
    people and technologies combine to work as heterogeneous but
    functional wholes’. See Edward Woodhouse and Jason W. Patton,
    ‘Design by Society: Science and Technology Studies and the Social
    Shaping of Design’, *Design Issues* 20.3 (2004): 1–12.

[^05NIshantShah_93]: Donna J. Haraway, *Simians, Cyborgs and Women: The Reinvention of
    Nature*, New York and London: Routledge, 1991, p. 180. I
    particularly want to reference Donna Haraway’s invocation of irony
    as a reading and a political strategy that allows for these
    contradictions to emerge as distinct and not necessarily in need of
    resolution.

[^05NIshantShah_94]: Haraway, *Simians, Cyborgs and Women*, p. 167.

[^05NIshantShah_95]: Jean Baudrillard, *Simulacra and Simulation*, trans. Sheila
    Glaser, Ann Arbor: University of Michigan Press, 1983.

[^05NIshantShah_96]: Gilles Deleuze, *Difference and Repetition*, trans. Paul Patton,
    Columbia: Columbia University Press, 1994, p. 69.

[^05NIshantShah_97]: Jaron Lanier, *You Are Not a Gadget: A Manifesto*, New York:
    Alfred A. Knopf, 2010.

[^05NIshantShah_98]: Malhotra, *Porn*.

[^05NIshantShah_99]: Duncan Watts, *Small Worlds: The Dynamics of Networks between
    Order and Randomness.* Princeton Studies in Complexity \#36.
    Princeton: Princeton University Press, 1999.

[^05NIshantShah_100]: Albert-László Barabási, ‘Scale-Free Networks: A Decade and
    Beyond’, *Science* 325.5939 (2009): 412–413*. *

[^05NIshantShah_101]: Barabási, ‘Scale-Free Networks’, p. 62.

[^05NIshantShah_102]: The Wire Staff does a sympathetic reconstruction of the sedition
    charges and arrest of Kumar and his other partners in their
    reporting. See ‘JNU Sedition Case: Umar Khalid, Kanhaiya Kumar,
    Other Accused Appear in Court’, *The Wire*, 16 March 2021,
    <a href="https://thewire.in/law/jnu-sedition-case-umar-khalid-kanhaiya-kumar-delhi-court">https://thewire.in/law/jnu-sedition-case-umar-khalid-kanhaiya-kumar-delhi-court</a>.

[^05NIshantShah_103]: Sarah Hafeez, ‘Zee News Producer Quits: Video We Shot Had No
    Pakistan Zindabad Slogan’, *The Indian Express*, 22 February 2016,
    <a href="https://indianexpress.com/article/india/india-news-india/zee-news-producer-quits-video-we-shot-had-no-pakistan-zindabad-slogan/">https://indianexpress.com/article/india/india-news-india/zee-news-producer-quits-video-we-shot-had-no-pakistan-zindabad-slogan/</a>.

[^05NIshantShah_104]: Rahul Kanwal, ‘JNU Row: Did a Fake Video Fuel the Anti-national
    Fire?’, *India Today*, 18 February 2016,
    <a href="https://www.indiatoday.in/india/story/panelists-debate-whether-kanhaiya-sedition-video-doctored-or-not-309451-2016-02-18">https://www.indiatoday.in/india/story/panelists-debate-whether-kanhaiya-sedition-video-doctored-or-not-309451-2016-02-18</a>.

[^05NIshantShah_105]: Neda Atanasoski and Kalindi Vora, *Surrogate Humanity:* *Race,
    Robots, and the Politics of Technological Futures*, Durham: Duke
    University Press, 2019, p. 12.

[^05NIshantShah_106]: Kate Crawford and Vladan Joler, ‘Anatomy of an AI System: The
    Amazon Echo as an Anatomical Map of Human Labor, Data and Planetary
    Resources’, *AI Now Institute and Share Lab*, 7 September 2018,
    <a href="https://anatomyof.ai">https://anatomyof.ai</a>.

[^05NIshantShah_107]: IANS, ‘Google Techie’s Lynching in Karnataka: How an Act of
    Kindness Turned Deadly’, *Business Standard*, 16 July 2018,
    <a href="https://www.business-standard.com/article/current-affairs/google-techie-s-lynching-in-karnataka-how-an-act-of-kindness-turned-deadly-118071500351%5C_1.html">https://www.business-standard.com/article/current-affairs/google-techie-s-lynching-in-karnataka-how-an-act-of-kindness-turned-deadly-118071500351\_1.html</a>.

[^05NIshantShah_108]: Reuters, ‘Bidar Lynching: He Looked Like terrorist, Says
    Villager, Days After WhatsApp Rumour Leads to Death of 1 in Rural
    Karnataka’, *Firstpost*, 30 July 2018,
    <a href="https://www.firstpost.com/india/bidar-lynching-he-looked-like-terrorist-says-villager-days-after-whatsapp-rumour-leads-to-death-of-1-in-rural-karnataka-4848181.html">https://www.firstpost.com/india/bidar-lynching-he-looked-like-terrorist-says-villager-days-after-whatsapp-rumour-leads-to-death-of-1-in-rural-karnataka-4848181.html</a>.

[^05NIshantShah_109]: Maria Manzano, *Extensions of First-Order Logic.* Cambridge
    Tracts in Theoretical Computer Science, Series \# 19. Cambridge:
    Cambridge University Press, 1996, p. 5.

[^05NIshantShah_110]: Clemens Apprich, Wendy Hui Kyong Chun, Florian Cramer, and Hito
    Steyerl, *Pattern Discrimination,* Minneapolis: University of
    Minnesota Press, 2019, p. 102.

[^05NIshantShah_111]: José van Dijck and Thomas Poell, ‘Understanding Social Media
    Logic’, *Media and Communication* 1.1 (2013): 9.

[^05NIshantShah_112]: Kristen Gelineau and Jon Gambrell, ‘New Zealand Mosque Shooter
    is a White Nationalist who Hates Immigrants, Documents and Video
    Reveal’, *Chicago Tribune*, 16 March 2019,
    <a href="https://www.chicagotribune.com/nation-world/ct-mosque-killer-white-supremacy-20190315-story.html">https://www.chicagotribune.com/nation-world/ct-mosque-killer-white-supremacy-20190315-story.html</a>.

[^05NIshantShah_113]: Charlotte Graham-McLay, ‘Spreading the Mosque Shooting Video is
    a Crime in New Zealand’, *The New York Times*, 21 March 2019,
    <a href="https://www.nytimes.com/2019/03/21/world/asia/new-zealand-attacks-social-media.html">https://www.nytimes.com/2019/03/21/world/asia/new-zealand-attacks-social-media.html</a>.

[^05NIshantShah_114]: BBC, ‘New Zealand Man Jailed for 21 Months for Sharing
    Christchurch Shooting Video’, *BBC News*, 18 June 2019,
    <a href="https://www.bbc.com/news/world-asia-48671837">https://www.bbc.com/news/world-asia-48671837</a>.

[^05NIshantShah_115]: Ben Strang, ‘Thousands Don’t Believe Official Christchurch
    Terror Attacks Story’, *RNZ*, 4 April 2019,
    <a href="https://www.rnz.co.nz/news/national/386367/thousands-don-t-believe-official-christchurch-terror-attacks-story">https://www.rnz.co.nz/news/national/386367/thousands-don-t-believe-official-christchurch-terror-attacks-story</a>.

